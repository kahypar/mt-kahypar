/*******************************************************************************
 * MIT License
 *
 * This file is part of Mt-KaHyPar.
 *
 * Copyright (C) 2020 Lars Gottesb√ºren <lars.gottesbueren@kit.edu>
 * Copyright (C) 2020 Tobias Heuer <tobias.heuer@kit.edu>
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 ******************************************************************************/

#include "local_moving_modularity.h"

#include "mt-kahypar/definitions.h"
#include "mt-kahypar/parallel/stl/thread_locals.h"
#include "mt-kahypar/utils/floating_point_comparisons.h"
#include "mt-kahypar/utils/timer.h"

#include <tbb/parallel_reduce.h>
#include <tbb/parallel_sort.h>

namespace mt_kahypar::metrics {
template <typename Hypergraph>
double modularity(const Graph<Hypergraph> &graph, const ds::Clustering &communities)
{
  ASSERT(graph.canBeUsed());
  ASSERT(graph.numNodes() == communities.size());
  vec<NodeID> nodes(graph.numNodes());
  vec<double> cluster_mod(graph.numNodes(), 0.0);

  // make summation order deterministic!
  tbb::parallel_for(UL(0), graph.numNodes(), [&](size_t pos) { nodes[pos] = pos; });
  tbb::parallel_sort(nodes.begin(), nodes.end(), [&](NodeID lhs, NodeID rhs) {
    return std::tie(communities[lhs], lhs) < std::tie(communities[rhs], rhs);
  });

  // deterministic reduce doesn't have dynamic load balancing --> precompute the
  // contributions and then sum them
  tbb::parallel_for(UL(0), graph.numNodes(), [&](size_t pos) {
    NodeID x = nodes[pos];
    PartitionID comm = communities[x];
    double comm_vol = 0.0, internal = 0.0;
    if(pos == 0 || communities[nodes[pos - 1]] != comm)
    {
      for(size_t i = pos; i < nodes.size() && communities[nodes[i]] == comm; ++i)
      {
        NodeID u = nodes[i];
        comm_vol += graph.nodeVolume(u);
        for(const Arc &arc : graph.arcsOf(u))
        {
          if(communities[arc.head] != comm)
          {
            internal -= arc.weight;
          }
        }
      }
      internal += comm_vol;
      cluster_mod[comm] = internal - (comm_vol * comm_vol) / graph.totalVolume();
    }
  });

  auto r = tbb::blocked_range<size_t>(UL(0), graph.numNodes(), 1000);
  auto combine_range = [&](const tbb::blocked_range<size_t> &r, double partial) {
    return std::accumulate(cluster_mod.begin() + r.begin(), cluster_mod.begin() + r.end(),
                           partial);
  };
  return tbb::parallel_deterministic_reduce(r, 0.0, combine_range, std::plus<>()) /
         graph.totalVolume();
}

namespace {
#define MODULARITY(X) double modularity(const Graph<X> &, const ds::Clustering &)
}

INSTANTIATE_FUNC_WITH_HYPERGRAPHS(MODULARITY)

}

namespace mt_kahypar::community_detection {

template <class Hypergraph>
bool ParallelLocalMovingModularity<Hypergraph>::localMoving(Graph<Hypergraph> &graph,
                                                            ds::Clustering &communities)
{
  ASSERT(graph.canBeUsed());
  _max_degree = graph.max_degree();
  _reciprocal_total_volume = 1.0 / graph.totalVolume();
  _vol_multiplier_div_by_node_vol = _reciprocal_total_volume;

  // init
  if(_context.partition.deterministic)
  {
    tbb::parallel_for(UL(0), graph.numNodes(), [&](NodeID u) {
      communities[u] = u;
      _cluster_volumes[u].store(graph.nodeVolume(u), std::memory_order_relaxed);
    });
  }
  else
  {
    auto &nodes = permutation.permutation;
    nodes.resize(graph.numNodes());
    tbb::parallel_for(0U, static_cast<NodeID>(graph.numNodes()), [&](const NodeID u) {
      nodes[u] = u;
      communities[u] = u;
      _cluster_volumes[u].store(graph.nodeVolume(u), std::memory_order_relaxed);
    });
  }

  DBG << "Louvain level" << V(graph.numNodes()) << V(graph.numArcs());

  // local moving
  bool clustering_changed = false;
  if(graph.numArcs() > 0)
  {
    size_t number_of_nodes_moved = graph.numNodes();
    for(size_t round = 0;
        number_of_nodes_moved >=
            _context.preprocessing.community_detection.min_vertex_move_fraction *
                graph.numNodes() &&
        round < _context.preprocessing.community_detection.max_pass_iterations;
        round++)
    {
      if(_context.partition.deterministic)
      {
        number_of_nodes_moved = synchronousParallelRound(graph, communities);
      }
      else
      {
        number_of_nodes_moved = parallelNonDeterministicRound(graph, communities);
      }
      clustering_changed |= number_of_nodes_moved > 0;
      DBG << "Louvain-Pass #" << round << " - num moves " << number_of_nodes_moved
          << " - Modularity:" << metrics::modularity(graph, communities);
    }
  }
  return clustering_changed;
}

template <class Hypergraph>
size_t ParallelLocalMovingModularity<Hypergraph>::synchronousParallelRound(
    const Graph<Hypergraph> &graph, ds::Clustering &communities)
{
  if(graph.numNodes() < 200)
  {
    return sequentialRound(graph, communities);
  }

  size_t seed = prng();
  permutation.random_grouping(
      graph.numNodes(), _context.shared_memory.static_balancing_work_packages, seed);
  size_t num_moved_nodes = 0;
  constexpr size_t num_buckets = utils::ParallelPermutation<HypernodeID>::num_buckets;
  const size_t num_sub_rounds =
      _context.preprocessing.community_detection.num_sub_rounds_deterministic;
  size_t num_buckets_per_sub_round =
      parallel::chunking::idiv_ceil(num_buckets, num_sub_rounds);

  size_t max_round_size = 0;
  for(size_t sub_round = 0; sub_round < num_sub_rounds; ++sub_round)
  {
    auto [first_bucket, last_bucket] =
        parallel::chunking::bounds(sub_round, num_buckets, num_buckets_per_sub_round);
    max_round_size =
        std::max(max_round_size, size_t(permutation.bucket_bounds[last_bucket] -
                                        permutation.bucket_bounds[first_bucket]));
  }
  volume_updates_to.adapt_capacity(max_round_size);
  volume_updates_from.adapt_capacity(max_round_size);

  for(size_t sub_round = 0; sub_round < num_sub_rounds; ++sub_round)
  {
    auto [first_bucket, last_bucket] =
        parallel::chunking::bounds(sub_round, num_buckets, num_buckets_per_sub_round);
    assert(first_bucket < last_bucket && last_bucket < permutation.bucket_bounds.size());
    size_t first = permutation.bucket_bounds[first_bucket];
    size_t last = permutation.bucket_bounds[last_bucket];

    tbb::enumerable_thread_specific<size_t> num_moved_local(0);
    tbb::parallel_for(first, last, [&](size_t pos) {
      HypernodeID u = permutation.at(pos);
      PartitionID best_cluster = computeMaxGainCluster(graph, communities, u);
      if(best_cluster != communities[u])
      {
        volume_updates_from.push_back_buffered({ communities[u], u });
        volume_updates_to.push_back_buffered({ best_cluster, u });
        num_moved_local.local() += 1;
      }
    });

    size_t num_moved_sub_round = num_moved_local.combine(std::plus<>());
    num_moved_nodes += num_moved_sub_round;

    // We can't do atomic adds of the volumes since they're not commutative and thus lead
    // to non-deterministic decisions Instead we sort the updates, and for each cluster
    // let one thread sum up the updates.
    tbb::parallel_invoke(
        [&] {
          volume_updates_to.finalize();
          tbb::parallel_sort(volume_updates_to.begin(), volume_updates_to.end());
        },
        [&] {
          volume_updates_from.finalize();
          tbb::parallel_sort(volume_updates_from.begin(), volume_updates_from.end());
        });

    const size_t sz_to = volume_updates_to.size();
    tbb::parallel_for(UL(0), sz_to, [&](size_t pos) {
      PartitionID c = volume_updates_to[pos].cluster;
      if(pos == 0 || volume_updates_to[pos - 1].cluster != c)
      {
        ArcWeight vol_delta = 0.0;
        for(; pos < sz_to && volume_updates_to[pos].cluster == c; ++pos)
        {
          vol_delta += graph.nodeVolume(volume_updates_to[pos].node);
          communities[volume_updates_to[pos].node] = c;
        }
        _cluster_volumes[c].store(_cluster_volumes[c].load(std::memory_order_relaxed) +
                                      vol_delta,
                                  std::memory_order_relaxed);
      }
    });
    volume_updates_to.clear();

    const size_t sz_from = volume_updates_from.size();
    tbb::parallel_for(UL(0), sz_from, [&](size_t pos) {
      PartitionID c = volume_updates_from[pos].cluster;
      if(pos == 0 || volume_updates_from[pos - 1].cluster != c)
      {
        ArcWeight vol_delta = 0.0;
        for(; pos < sz_from && volume_updates_from[pos].cluster == c; ++pos)
        {
          vol_delta -= graph.nodeVolume(volume_updates_from[pos].node);
        }
        _cluster_volumes[c].store(_cluster_volumes[c].load(std::memory_order_relaxed) +
                                      vol_delta,
                                  std::memory_order_relaxed);
      }
    });
    volume_updates_from.clear();
  }

  return num_moved_nodes;
}

template <class Hypergraph>
size_t
ParallelLocalMovingModularity<Hypergraph>::sequentialRound(const Graph<Hypergraph> &graph,
                                                           ds::Clustering &communities)
{
  size_t seed = prng();
  permutation.sequential_fallback(graph.numNodes(), seed);
  size_t num_moved = 0;
  for(size_t i = 0; i < graph.numNodes(); ++i)
  {
    NodeID u = permutation.at(i);
    PartitionID best_cluster = computeMaxGainCluster(graph, communities, u);
    if(best_cluster != communities[u])
    {
      _cluster_volumes[best_cluster] += graph.nodeVolume(u);
      _cluster_volumes[communities[u]] -= graph.nodeVolume(u);
      communities[u] = best_cluster;
      num_moved++;
    }
  }
  return num_moved;
}

template <class Hypergraph>
size_t ParallelLocalMovingModularity<Hypergraph>::parallelNonDeterministicRound(
    const Graph<Hypergraph> &graph, ds::Clustering &communities)
{
  auto &nodes = permutation.permutation;
  if(!_disable_randomization)
  {
    utils::Randomize::instance().parallelShuffleVector(nodes, UL(0), nodes.size());
  }

  tbb::enumerable_thread_specific<size_t> local_number_of_nodes_moved(0);
  auto moveNode = [&](const NodeID u) {
    const ArcWeight volU = graph.nodeVolume(u);
    const PartitionID from = communities[u];
    PartitionID best_cluster = computeMaxGainCluster(graph, communities, u);
    if(best_cluster != from)
    {
      _cluster_volumes[best_cluster] += volU;
      _cluster_volumes[from] -= volU;
      communities[u] = best_cluster;
      ++local_number_of_nodes_moved.local();
    }
  };

#ifdef KAHYPAR_ENABLE_HEAVY_PREPROCESSING_ASSERTIONS
  std::for_each(nodes.begin(), nodes.end(), moveNode);
#else
  tbb::parallel_for(UL(0), nodes.size(), [&](size_t i) { moveNode(nodes[i]); });
#endif
  size_t number_of_nodes_moved = local_number_of_nodes_moved.combine(std::plus<>());
  return number_of_nodes_moved;
}

template <class Hypergraph>
bool ParallelLocalMovingModularity<Hypergraph>::verifyGain(
    const Graph<Hypergraph> &graph, const ds::Clustering &communities, const NodeID u,
    const PartitionID to, double gain, double weight_from, double weight_to)
{
  if(_context.partition.deterministic)
  {
    // the check is omitted, since changing the cluster volumes breaks determinism
    return true;
  }

  const PartitionID from = communities[u];

  long double adjustedGain = adjustAdvancedModGain(
      gain, weight_from, _cluster_volumes[from], graph.nodeVolume(u));
  const double volMultiplier = _vol_multiplier_div_by_node_vol * graph.nodeVolume(u);
  double modGain = modularityGain(weight_to, _cluster_volumes[to], volMultiplier);
  long double adjustedGainRecomputed = adjustAdvancedModGain(
      modGain, weight_from, _cluster_volumes[from], graph.nodeVolume(u));
  unused(adjustedGainRecomputed);

  if(from == to)
  {
    adjustedGainRecomputed = 0.0L;
    adjustedGain = 0.0L;
  }

  ASSERT(adjustedGain == adjustedGainRecomputed);

  long double dTotalVolumeSquared = static_cast<long double>(graph.totalVolume()) *
                                    static_cast<long double>(graph.totalVolume());

  auto accBeforeMove =
      intraClusterWeightsAndSumOfSquaredClusterVolumes(graph, communities);
  long double coverageBeforeMove =
      static_cast<long double>(accBeforeMove.first) / graph.totalVolume();
  long double expectedCoverageBeforeMove = accBeforeMove.second / dTotalVolumeSquared;
  long double modBeforeMove = coverageBeforeMove - expectedCoverageBeforeMove;

  // apply move
  ds::Clustering communities_after_move = communities;
  communities_after_move[u] = to;
  _cluster_volumes[to] += graph.nodeVolume(u);
  _cluster_volumes[from] -= graph.nodeVolume(u);

  auto accAfterMove =
      intraClusterWeightsAndSumOfSquaredClusterVolumes(graph, communities_after_move);
  long double coverageAfterMove =
      static_cast<long double>(accAfterMove.first) / graph.totalVolume();
  long double expectedCoverageAfterMove = accAfterMove.second / dTotalVolumeSquared;
  long double modAfterMove = coverageAfterMove - expectedCoverageAfterMove;

  const bool result =
      math::are_almost_equal_ld(modBeforeMove + adjustedGain, modAfterMove, 1e-8);
  ASSERT(result, V(modBeforeMove + adjustedGain)
                     << V(modAfterMove) << V(gain) << V(adjustedGain)
                     << V(coverageBeforeMove) << V(expectedCoverageBeforeMove)
                     << V(modBeforeMove) << V(coverageAfterMove)
                     << V(expectedCoverageAfterMove) << V(modAfterMove));

  _cluster_volumes[to] -= graph.nodeVolume(u);
  _cluster_volumes[from] += graph.nodeVolume(u);

  return result;
}

template <class Hypergraph>
std::pair<ArcWeight, ArcWeight> ParallelLocalMovingModularity<Hypergraph>::
    intraClusterWeightsAndSumOfSquaredClusterVolumes(const Graph<Hypergraph> &graph,
                                                     const ds::Clustering &communities)
{
  ArcWeight intraClusterWeights = 0;
  ArcWeight sumOfSquaredClusterVolumes = 0;
  vec<ArcWeight> cluster_volumes(graph.numNodes(), 0);

  for(NodeID u : graph.nodes())
  {
    ArcWeight arcVol = 0;
    for(const Arc &arc : graph.arcsOf(u))
    {
      if(communities[u] == communities[arc.head])
        intraClusterWeights += arc.weight;
      arcVol += arc.weight;
    }

    ArcWeight selfLoopWeight =
        graph.nodeVolume(u) - arcVol; // already accounted for as twice!
    ASSERT(selfLoopWeight >= 0.0);
    intraClusterWeights += selfLoopWeight;
    cluster_volumes[communities[u]] += graph.nodeVolume(u);
  }

  for(NodeID cluster : graph.nodes())
  { // unused cluster IDs have volume 0
    sumOfSquaredClusterVolumes += cluster_volumes[cluster] * cluster_volumes[cluster];
  }
  return std::make_pair(intraClusterWeights, sumOfSquaredClusterVolumes);
}

template <class Hypergraph>
void ParallelLocalMovingModularity<Hypergraph>::initializeClusterVolumes(
    const Graph<Hypergraph> &graph, ds::Clustering &communities)
{
  _reciprocal_total_volume = 1.0 / graph.totalVolume();
  _vol_multiplier_div_by_node_vol = _reciprocal_total_volume;
  tbb::parallel_for(0U, static_cast<NodeID>(graph.numNodes()), [&](const NodeID u) {
    const PartitionID community_id = communities[u];
    _cluster_volumes[community_id] += graph.nodeVolume(u);
  });
}

template <class Hypergraph>
ParallelLocalMovingModularity<Hypergraph>::~ParallelLocalMovingModularity()
{
  /*
    tbb::parallel_invoke([&] {
      parallel::parallel_free_thread_local_internal_data(
              _local_small_incident_cluster_weight,
    [&](CacheEfficientIncidentClusterWeights& data) { data.freeInternalData();
              });
    }, [&] {
      parallel::parallel_free_thread_local_internal_data(
              _local_large_incident_cluster_weight, [&](LargeIncidentClusterWeights& data)
    { data.freeInternalData();
              });
    }, [&] {
       parallel::free(_cluster_volumes);
    });
  */
}

INSTANTIATE_CLASS_WITH_HYPERGRAPHS(ParallelLocalMovingModularity)

}