/*******************************************************************************
 * MIT License
 *
 * This file is part of Mt-KaHyPar.
 *
 * Copyright (C) 2021 Tobias Heuer <tobias.heuer@kit.edu>
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 ******************************************************************************/

#pragma once

#include "tbb/enumerable_thread_specific.h"

#include "mt-kahypar/datastructures/sparse_map.h"
#include "mt-kahypar/parallel/atomic_wrapper.h"
#include "mt-kahypar/parallel/stl/scalable_queue.h"
#include "mt-kahypar/parallel/stl/scalable_vector.h"
#include "mt-kahypar/partition/context.h"
#include "mt-kahypar/partition/refinement/flows/quotient_graph.h"
#include "mt-kahypar/partition/refinement/flows/refiner_adapter.h"

namespace mt_kahypar {

template <typename TypeTraits>
class ProblemConstruction
{

  static constexpr bool debug = false;

  using PartitionedHypergraph = typename TypeTraits::PartitionedHypergraph;

  /**
   * Contains data required to grow two region around
   * the cut of two blocks of the partition.
   */
  struct BFSData
  {
    explicit BFSData(const HypernodeID num_nodes, const HyperedgeID num_edges,
                     const PartitionID k) :
        current_distance(0),
        queue(), next_queue(), visited_hn(num_nodes, false), visited_he(num_edges, false),
        contained_hes(num_edges, false), locked_blocks(k, false), queue_weight_block_0(0),
        queue_weight_block_1(0), lock_queue(false)
    {
    }

    void clearQueue();

    void reset();

    HypernodeID pop_hypernode();

    void add_pins_of_hyperedge_to_queue(const HyperedgeID &he,
                                        const PartitionedHypergraph &phg,
                                        const size_t max_bfs_distance,
                                        const HypernodeWeight max_weight_block_0,
                                        const HypernodeWeight max_weight_block_1);

    bool is_empty() const { return queue.empty(); }

    bool is_next_empty() const { return next_queue.empty(); }

    void swap_with_next_queue()
    {
      if(!is_next_empty())
      {
        std::swap(queue, next_queue);
        ++current_distance;
      }
    }

    BlockPair blocks;
    size_t current_distance;
    parallel::scalable_queue<HypernodeID> queue;
    parallel::scalable_queue<HypernodeID> next_queue;
    vec<bool> visited_hn;
    vec<bool> visited_he;
    vec<bool> contained_hes;
    vec<bool> locked_blocks;
    HypernodeWeight queue_weight_block_0;
    HypernodeWeight queue_weight_block_1;
    bool lock_queue;
  };

public:
  explicit ProblemConstruction(const HypernodeID num_hypernodes,
                               const HyperedgeID num_hyperedges, const Context &context) :
      _context(context),
      _scaling(1.0 + _context.refinement.flows.alpha *
                         std::min(0.05, _context.partition.epsilon)),
      _num_hypernodes(num_hypernodes), _num_hyperedges(num_hyperedges), _local_bfs([&] {
        // If the number of blocks changes, BFSData needs to be initialized
        // differently. Thus we use a lambda that reads the current number of
        // blocks from the context
        return constructBFSData();
      })
  {
  }

  ProblemConstruction(const ProblemConstruction &) = delete;
  ProblemConstruction(ProblemConstruction &&) = delete;

  ProblemConstruction &operator=(const ProblemConstruction &) = delete;
  ProblemConstruction &operator=(ProblemConstruction &&) = delete;

  Subhypergraph construct(const SearchID search_id,
                          QuotientGraph<TypeTraits> &quotient_graph,
                          const PartitionedHypergraph &phg);

  void changeNumberOfBlocks(const PartitionID new_k);

private:
  BFSData constructBFSData() const
  {
    return BFSData(_num_hypernodes, _num_hyperedges, _context.partition.k);
  }

  MT_KAHYPAR_ATTRIBUTE_ALWAYS_INLINE bool isMaximumProblemSizeReached(
      const Subhypergraph &sub_hg, const HypernodeWeight max_weight_block_0,
      const HypernodeWeight max_weight_block_1, vec<bool> &locked_blocks) const;

  const Context &_context;
  double _scaling;
  HypernodeID _num_hypernodes;
  HyperedgeID _num_hyperedges;

  // ! Contains data required for BFS construction algorithm
  tbb::enumerable_thread_specific<BFSData> _local_bfs;
};

} // namespace kahypar
