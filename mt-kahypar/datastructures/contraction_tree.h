/*******************************************************************************
 * MIT License
 *
 * This file is part of Mt-KaHyPar.
 *
 * Copyright (C) 2020 Lars Gottesb√ºren <lars.gottesbueren@kit.edu>
 * Copyright (C) 2020 Tobias Heuer <tobias.heuer@kit.edu>
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 ******************************************************************************/

#pragma once

#include <tbb/parallel_for.h>

#include "mt-kahypar/datastructures/array.h"
#include "mt-kahypar/datastructures/hypergraph_common.h"
#include "mt-kahypar/parallel/atomic_wrapper.h"
#include "mt-kahypar/parallel/stl/scalable_vector.h"
#include "mt-kahypar/utils/memory_tree.h"
#include "mt-kahypar/utils/range.h"

namespace mt_kahypar {
namespace ds {

// Represents a uncontraction that is assigned to a certain batch
// and within that batch to a certain position.
struct BatchAssignment
{
  HypernodeID u;
  HypernodeID v;
  size_t batch_index;
  size_t batch_pos;
};

/*!
 * Helper class that synchronizes assignements of uncontractions
 * to batches. A batch has a certain maximum allowed batch size. The
 * class provides functionality to compute such an assignment in a
 * thread-safe manner. Several threads can request a batch index
 * and a position within that batch for its uncontraction it wants
 * to assign. The class guarantees that each combination of
 * (batch_index, batch_position) is unique and consecutive.
 * Furthermore, it is ensured that batch_position is always
 * smaller than max_batch_size.
 */
class BatchIndexAssigner
{

  using AtomicCounter = parallel::IntegralAtomicWrapper<size_t>;

public:
  explicit BatchIndexAssigner(const HypernodeID num_hypernodes,
                              const size_t max_batch_size) :
      _max_batch_size(max_batch_size),
      _high_water_mark(0), _current_batch_counter(num_hypernodes, AtomicCounter(0)),
      _current_batch_sizes(num_hypernodes, AtomicCounter(0))
  {
  }

  BatchAssignment getBatchIndex(const size_t min_required_batch,
                                const size_t num_uncontractions = 1)
  {
    if(min_required_batch <= _high_water_mark)
    {
      size_t current_high_water_mark = _high_water_mark.load();
      const BatchAssignment assignment =
          findBatchAssignment(current_high_water_mark, num_uncontractions);

      // Update high water mark in case batch index is greater than
      // current high water mark
      size_t current_batch_index = assignment.batch_index;
      increaseHighWaterMark(current_batch_index);
      return assignment;
    }
    else
    {
      return findBatchAssignment(min_required_batch, num_uncontractions);
    }
  }

  size_t batchSize(const size_t batch_index) const
  {
    ASSERT(batch_index < _current_batch_sizes.size());
    return _current_batch_sizes[batch_index];
  }

  void increaseHighWaterMark(size_t new_high_water_mark)
  {
    size_t current_high_water_mark = _high_water_mark.load();
    while(new_high_water_mark > current_high_water_mark)
    {
      _high_water_mark.compare_exchange_strong(current_high_water_mark,
                                               new_high_water_mark);
    }
  }

  size_t numberOfNonEmptyBatches()
  {
    size_t current_batch = _high_water_mark;
    if(_current_batch_sizes[_high_water_mark] == 0)
    {
      while(current_batch > 0 && _current_batch_sizes[current_batch] == 0)
      {
        --current_batch;
      }
      if(_current_batch_sizes[current_batch] > 0)
      {
        ++current_batch;
      }
    }
    else
    {
      while(_current_batch_sizes[current_batch] > 0)
      {
        ++current_batch;
      }
    }
    return current_batch;
  }

  void reset(const size_t num_batches)
  {
    ASSERT(num_batches <= _current_batch_sizes.size());
    _high_water_mark = 0;
    tbb::parallel_for(UL(0), num_batches, [&](const size_t i) {
      _current_batch_counter[i] = 0;
      _current_batch_sizes[i] = 0;
    });
  }

private:
  BatchAssignment findBatchAssignment(const size_t start_batch_index,
                                      const size_t num_uncontractions)
  {
    size_t current_batch_index = start_batch_index;
    size_t batch_pos = _current_batch_counter[current_batch_index].fetch_add(
        num_uncontractions, std::memory_order_relaxed);
    // Search for batch in which atomic update of the batch counter
    // return a position smaller than max_batch_size.
    while(batch_pos >= _max_batch_size)
    {
      ++current_batch_index;
      ASSERT(current_batch_index < _current_batch_counter.size());
      batch_pos = _current_batch_counter[current_batch_index].fetch_add(
          num_uncontractions, std::memory_order_relaxed);
    }
    ASSERT(batch_pos < _max_batch_size);
    _current_batch_sizes[current_batch_index] += num_uncontractions;
    return BatchAssignment{ kInvalidHypernode, kInvalidHypernode, current_batch_index,
                            batch_pos };
  }

  const size_t _max_batch_size;
  AtomicCounter _high_water_mark;
  parallel::scalable_vector<AtomicCounter> _current_batch_counter;
  parallel::scalable_vector<AtomicCounter> _current_batch_sizes;
};

class ContractionTree
{

  static constexpr bool debug = false;
  static constexpr bool enable_heavy_assert = false;
  static constexpr size_t kInvalidVersion = std::numeric_limits<size_t>::max();

  using Timepoint = HypernodeID;

public:
  struct Interval
  {
    explicit Interval() : start(kInvalidHypernode), end(kInvalidHypernode) {}

    Timepoint start;
    Timepoint end;
  };

private:
  /**
   * Represents a node in contraction tree and contains all information
   * associated with that node.
   */
  class Node
  {
  public:
    Node() :
        _parent(0), _pending_contractions(0), _subtree_size(0), _version(kInvalidVersion),
        _interval()
    {
    }

    inline HypernodeID parent() const { return _parent; }

    inline void setParent(const HypernodeID parent) { _parent = parent; }

    inline HypernodeID pendingContractions() const { return _pending_contractions; }

    inline void incrementPendingContractions() { ++_pending_contractions; }

    inline void decrementPendingContractions() { --_pending_contractions; }

    inline HypernodeID subtreeSize() const { return _subtree_size; }

    inline void setSubtreeSize(const HypernodeID subtree_size)
    {
      _subtree_size = subtree_size;
    }

    inline size_t version() const { return _version; }

    inline void setVersion(const size_t version) { _version = version; }

    inline Interval interval() const { return _interval; }

    inline void setInterval(const Timepoint start, const Timepoint end)
    {
      ASSERT(start < end);
      _interval.start = start;
      _interval.end = end;
    }

    inline void reset(const HypernodeID u)
    {
      _parent = u;
      _pending_contractions = 0;
      _subtree_size = 0;
      _version = kInvalidVersion;
      _interval.start = kInvalidHypernode;
      _interval.end = kInvalidHypernode;
    }

  private:
    // ! Parent in the contraction tree
    HypernodeID _parent;
    // ! Number of pending contractions
    HypernodeID _pending_contractions;
    // ! Size of the subtree
    HypernodeID _subtree_size;
    // ! Version number of the hypergraph for which contract the corresponding vertex
    size_t _version;
    // ! "Time" interval on which the contraction of this node takes place
    Interval _interval;
  };

  static_assert(std::is_trivially_copyable<Node>::value,
                "Node is not trivially copyable");

public:
  // ! Iterator to iterate over the childs of a tree node
  using ChildIterator = typename parallel::scalable_vector<HypernodeID>::const_iterator;

  explicit ContractionTree() :
      _num_hypernodes(0), _finalized(false), _tree(), _roots(), _version_roots(),
      _out_degrees(), _incidence_array()
  {
  }

  ContractionTree(ContractionTree &&other) :
      _num_hypernodes(other._num_hypernodes), _finalized(other._finalized),
      _tree(std::move(other._tree)), _roots(std::move(other._roots)),
      _version_roots(std::move(other._version_roots)),
      _out_degrees(std::move(other._out_degrees)),
      _incidence_array(std::move(other._incidence_array))
  {
  }

  ContractionTree &operator=(ContractionTree &&other)
  {
    _num_hypernodes = other._num_hypernodes;
    _finalized = other._finalized;
    _tree = std::move(other._tree);
    _roots = std::move(other._roots);
    _version_roots = std::move(other._version_roots);
    _out_degrees = std::move(other._out_degrees);
    _incidence_array = std::move(other._incidence_array);
    return *this;
  }

  ~ContractionTree() { freeInternalData(); }

  // ####################### Tree Node Information #######################

  HypernodeID num_hypernodes() const { return _num_hypernodes; }

  // ! Returns the parent of node u
  MT_KAHYPAR_ATTRIBUTE_ALWAYS_INLINE HypernodeID parent(const HypernodeID u) const
  {
    return node(u).parent();
  }

  // ! Number of pending contractions of node u
  MT_KAHYPAR_ATTRIBUTE_ALWAYS_INLINE HypernodeID
  pendingContractions(const HypernodeID u) const
  {
    return node(u).pendingContractions();
  }

  // ! Subtree size of node u
  HypernodeID subtreeSize(const HypernodeID u) const
  {
    ASSERT(_finalized, "Information currently not available");
    return node(u).subtreeSize();
  }

  size_t version(const HypernodeID u) const
  {
    ASSERT(u < _num_hypernodes, "Hypernode" << u << "does not exist");
    return _tree[u].version();
  }

  // ! Degree/Number of childs of node u
  HypernodeID degree(const HypernodeID u) const
  {
    ASSERT(_finalized, "Information currently not available");
    ASSERT(u < _num_hypernodes, "Hypernode" << u << "does not exist");
    return _out_degrees[u + 1] - _out_degrees[u];
  }

  const parallel::scalable_vector<HypernodeID> &roots() const { return _roots; }

  const parallel::scalable_vector<HypernodeID> &
  roots_of_version(const size_t version) const
  {
    ASSERT(version < _version_roots.size());
    return _version_roots[version];
  }

  Interval interval(const HypernodeID u) const
  {
    ASSERT(u < _num_hypernodes, "Hypernode" << u << "does not exist");
    return node(u).interval();
  }

  // ####################### Iterators #######################

  // ! Returns a range to loop over the childs of a tree node u.
  IteratorRange<ChildIterator> childs(const HypernodeID u) const
  {
    ASSERT(_finalized, "Information currently not available");
    ASSERT(u < _num_hypernodes, "Hypernode" << u << "does not exist");
    return IteratorRange<ChildIterator>(_incidence_array.cbegin() + _out_degrees[u],
                                        _incidence_array.cbegin() + _out_degrees[u + 1]);
  }

  // ! Calls function f for each child of vertex u with the corresponding version
  template <typename F>
  void doForEachChildOfVersion(const HypernodeID u, const size_t version,
                               const F &f) const
  {
    ASSERT(_finalized, "Information currently not available");
    ASSERT(u < _num_hypernodes, "Hypernode" << u << "does not exist");
    for(const HypernodeID &v : childs(u))
    {
      if(_tree[v].version() == version)
      {
        f(v);
      }
    }
  }

  // ####################### Contraction Functions #######################

  // ! Registers a contraction in the contraction tree
  void registerContraction(const HypernodeID u, const HypernodeID v,
                           const size_t version = 0)
  {
    node(u).incrementPendingContractions();
    node(v).setParent(u);
    node(v).setVersion(version);
  }

  template <typename A, typename R>
  bool registerContraction(const HypernodeID u, const HypernodeID v, const size_t version,
                           A acquire, R release)
  {
    // Acquires ownership of vertex v that gives the calling thread exclusive rights
    // to modify the contraction tree entry of v
    acquire(v);

    // If there is no other contraction registered for vertex v
    // we try to determine its representative in the contraction tree
    if(parent(v) == v)
    {

      HypernodeID w = u;
      bool cycle_detected = false;
      while(true)
      {
        // Search for representative of u in the contraction tree.
        // It is either a root of the contraction tree or a vertex
        // with a reference count greater than zero, which indicates
        // that there are still ongoing contractions on this node that
        // have to be processed.
        while(parent(w) != w && pendingContractions(w) == 0)
        {
          w = parent(w);
          if(w == v)
          {
            cycle_detected = true;
            break;
          }
        }

        if(!cycle_detected)
        {
          // In case contraction of u and v does not induce any
          // cycle in the contraction tree we try to acquire vertex w
          if(w < v)
          {
            // Acquire ownership in correct order to prevent deadlocks
            release(v);
            acquire(w);
            acquire(v);
            if(parent(v) != v)
            {
              release(v);
              release(w);
              return false;
            }
          }
          else
          {
            acquire(w);
          }

          // Double-check condition of while loop above after acquiring
          // ownership of w
          if(parent(w) != w && pendingContractions(w) == 0)
          {
            // In case something changed, we release ownership of w and
            // search again for the representative of u.
            release(w);
          }
          else
          {
            // Otherwise we perform final cycle check to verify that
            // contraction of u and v will not introduce any new cycle.
            HypernodeID x = w;
            do
            {
              x = parent(x);
              if(x == v)
              {
                cycle_detected = true;
                break;
              }
            } while(parent(x) != x);

            if(cycle_detected)
            {
              release(w);
              release(v);
              return false;
            }

            // All checks succeded, we can safely increment the
            // reference count of w and update the contraction tree
            break;
          }
        }
        else
        {
          release(v);
          return false;
        }
      }

      // Increment reference count of w indicating that there pending
      // contraction at vertex w and update contraction tree.
      registerContraction(w, v, version);

      release(w);
      release(v);
      return true;
    }
    else
    {
      release(v);
      return false;
    }
  }

  // ! Unregisters a contraction in the contraction tree
  void unregisterContraction(const HypernodeID u, const HypernodeID v,
                             const Timepoint start, const Timepoint end,
                             const bool failed = false)
  {
    ASSERT(node(v).parent() == u, "Node" << u << "is not parent of node" << v);
    ASSERT(node(u).pendingContractions() > 0,
           "There are no pending contractions for node" << u);
    node(u).decrementPendingContractions();
    if(failed)
    {
      node(v).setParent(v);
      node(v).setVersion(kInvalidVersion);
    }
    else
    {
      node(v).setInterval(start, end);
    }
  }

  BatchVector
  createBatchUncontractionHierarchyForVersion(BatchIndexAssigner &batch_assigner,
                                              const size_t version);

  // ! Only for testing
  void setParent(const HypernodeID u, const HypernodeID v, const size_t version = 0)
  {
    node(u).setParent(v);
    node(u).setVersion(version);
  }

  // ! Only for testing
  void decrementPendingContractions(const HypernodeID u)
  {
    node(u).decrementPendingContractions();
  }

  // ####################### Initialize / Finalize #######################

  // ! Initializes the data structure in parallel
  void initialize(const HypernodeID num_hypernodes);

  // ! Finalizes the contraction tree which involve reversing the parent pointers
  // ! such that the contraction tree can be traversed in a top-down fashion and
  // ! computing the subtree sizes.
  void finalize(const size_t num_versions = 1);

  // ####################### Copy #######################

  // ! Copy contraction tree in parallel
  ContractionTree copy(parallel_tag_t) const;

  // ! Copy contraction tree sequentially
  ContractionTree copy() const;

  // ! Resets internal data structures
  void reset();

  // ! Free internal data in parallel
  void freeInternalData();

  void memoryConsumption(utils::MemoryTreeNode *parent) const;

private:
  // ! Accessor for contraction tree-related information
  MT_KAHYPAR_ATTRIBUTE_ALWAYS_INLINE const Node &node(const HypernodeID u) const
  {
    ASSERT(u < _num_hypernodes, "Hypernode" << u << "does not exist");
    return _tree[u];
  }

  // ! To avoid code duplication we implement non-const version in terms of const version
  MT_KAHYPAR_ATTRIBUTE_ALWAYS_INLINE Node &node(const HypernodeID u)
  {
    return const_cast<Node &>(static_cast<const ContractionTree &>(*this).node(u));
  }

  bool verifyBatchIndexAssignments(
      const BatchIndexAssigner &batch_assigner,
      const parallel::scalable_vector<parallel::scalable_vector<BatchAssignment> >
          &local_batch_assignments) const;

  HypernodeID _num_hypernodes;
  bool _finalized;
  parallel::scalable_vector<Node> _tree;
  parallel::scalable_vector<HypernodeID> _roots;
  parallel::scalable_vector<parallel::scalable_vector<HypernodeID> > _version_roots;
  parallel::scalable_vector<parallel::IntegralAtomicWrapper<HypernodeID> > _out_degrees;
  parallel::scalable_vector<HypernodeID> _incidence_array;
};

} // namespace ds
} // namespace mt_kahypar
