{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33826cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "import benchmark_visualization as bv\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from statistics import geometric_mean, mean\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import LogLocator, ScalarFormatter, FixedLocator\n",
    "import matplot2tikz as tikzplotlib\n",
    "\n",
    "\n",
    "# --- Academic Style Configuration ---\n",
    "def set_academic_style():\n",
    "    # Attempt to use LaTeX for text rendering (requires local latex install)\n",
    "    # If this fails, set text.usetex to False\n",
    "    try:\n",
    "        plt.rcParams.update({\n",
    "            \"text.usetex\": True,\n",
    "            \"font.family\": \"serif\",\n",
    "            \"font.serif\": [\"Computer Modern Roman\"],\n",
    "        })\n",
    "    except:\n",
    "        print(\"Warning: LaTeX not found. Text rendering will use standard fonts.\")\n",
    "        plt.rcParams.update({\"text.usetex\": False, \"font.family\": \"serif\"})\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        # Font sizes\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 12,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \n",
    "        # Line styles\n",
    "        \"lines.linewidth\": 1.5,\n",
    "        \"lines.markersize\": 0, # Usually academic plots rely on lines, markers often clutter dense time series\n",
    "        \n",
    "        # Grid (Reference image has a light dotted grid)\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.alpha\": 0.5,\n",
    "        \"grid.linestyle\": \":\",\n",
    "        \"grid.linewidth\": 0.8,\n",
    "        \n",
    "        # Legend (Reference: No frame, inside plot)\n",
    "        \"legend.frameon\": False,\n",
    "        \"legend.loc\": \"lower left\",\n",
    "        \"legend.borderpad\": 0.2,\n",
    "    })\n",
    "\n",
    "# Define the custom color palette based on the reference image (Green, Blue, Brown, Teal, Orange, Purple, Red, Black)\n",
    "ACADEMIC_COLORS = [\n",
    "    \"#4DAF4A\", # Green (KaHyPar-CA)\n",
    "    \"#377EB8\", # Blue (KaHyPar-CA-V)\n",
    "    \"#A65628\", # Brown\n",
    "    \"#98E0D6\", # Teal/Light Blue\n",
    "    \"#FF7F00\", # Orange\n",
    "    \"#984EA3\", # Purple\n",
    "    \"#E41A1C\", # Red\n",
    "    \"#000000\", # Black\n",
    "]\n",
    "\n",
    "set_academic_style()\n",
    "\n",
    "HOME_DIR = os.environ['HOME']\n",
    "\n",
    "SHORT_TIMELIMIT = 7200\n",
    "LONG_TIMELIMIT = 28800\n",
    "\n",
    "# Default figure save directory (user may override per call)\n",
    "FIG_SAVE_DIR = os.path.join(HOME_DIR, 'Documents', 'BA_benchmarks')\n",
    "\n",
    "runs = {}\n",
    "class BenchmarkRun:\n",
    "    HEADER = \"algorithm,graph,timeout,seed,k,epsilon,num_threads,imbalance,totalPartitionTime,objective,km1,cut,failed\"\n",
    "    \n",
    "    def __init__(self, content: str, timelimit: int = None):\n",
    "        self.data = {}\n",
    "        for i, key in enumerate(self.HEADER.split(',')):\n",
    "            value = content.split(',')[i].strip()\n",
    "            # Try to convert to int or float if possible\n",
    "            try:\n",
    "                if '.' in value:\n",
    "                    value = float(value)\n",
    "                else:\n",
    "                    value = int(value)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            self.data[key] = value\n",
    "        self.data['timelimit'] = timelimit\n",
    "\n",
    "    def get(self, param):\n",
    "        return self.data[param]\n",
    "\n",
    "def parse_results_file(file_path: str):\n",
    "    results_array = []\n",
    "    timelimit = file_path.split('.')[-2]\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            results_array += [BenchmarkRun(content=line, timelimit=int(timelimit))]\n",
    "    return results_array\n",
    "\n",
    "def aggregate_runs(directory_path: str, instances_to_exclude: list = []):\n",
    "    runs = {}\n",
    "    for file_path in glob.glob(directory_path + \"/*.results\"):\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        instance_name = '.'.join(file_name.split('.')[:-2])\n",
    "        purely_instance_name = '.'.join(instance_name.split('.')[:-3])\n",
    "        if purely_instance_name in instances_to_exclude:\n",
    "            continue   \n",
    "        if instance_name not in runs:\n",
    "            runs[instance_name] = {\"short\": None, \"long\": None}\n",
    "\n",
    "        run_results = parse_results_file(file_path)\n",
    "        if len(run_results) > 1:\n",
    "            min_km1=min([run.get('km1') for run in run_results if run.get('failed') == 'no'])\n",
    "            runs[instance_name][\"short\"] = min_km1\n",
    "        elif run_results:\n",
    "            runs[instance_name][\"long\"] = run_results[0].get('km1')\n",
    "        else:\n",
    "            pass\n",
    "            #print(f\"Warning: No results in file {file_path}\")\n",
    "\n",
    "    return runs\n",
    "\n",
    "\n",
    "def convert_instance_naming_scheme(instance_name: str, use_fixed_seed: bool) -> str:\n",
    "    parts = instance_name.split('.')\n",
    "    hgr_index = parts.index('hgr')\n",
    "    base_name = '.'.join(parts[:hgr_index + 1])\n",
    "    \n",
    "    # Extract parameters from the rest\n",
    "    k_value = None\n",
    "    seed_value = None\n",
    "    timelimit_value = None\n",
    "\n",
    "    for part in parts[hgr_index + 1:]:\n",
    "        if part.startswith('k'):\n",
    "            k_value = part[1:]\n",
    "        elif part.startswith('seed'):\n",
    "            seed_value = part[4:]\n",
    "        elif part.startswith('timelimit'):\n",
    "            timelimit_value = part[9:]\n",
    "    \n",
    "    # Construct new name: base.threads.k.seed.timelimit\n",
    "    threads = '1'  # Default to 1 thread if not specified\n",
    "    if use_fixed_seed:\n",
    "        seed_value = '1'\n",
    "    new_name = f\"{base_name}.{threads}.{k_value}.{seed_value}.{timelimit_value}\"\n",
    "    return new_name\n",
    "\n",
    "\n",
    "def parse_end_result_history_file(file_path: str):\n",
    "    # Read the last number from the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            last_line = lines[-1].strip()\n",
    "            km1 = int(last_line.split(',')[-1].strip())\n",
    "        else:\n",
    "            km1 = None\n",
    "    return km1\n",
    "\n",
    "def parse_history_file(file_path: str):\n",
    "    history = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue         \n",
    "            if line.startswith('Starttime:'):\n",
    "                timestamp = int(line.split(':')[1].strip())\n",
    "                history.append((timestamp, None, None))\n",
    "            else:\n",
    "                parts = line.split(',')\n",
    "                timestamp = int(parts[0].strip())\n",
    "                mode = parts[1].strip()\n",
    "                km1_value = int(parts[2].strip())\n",
    "                history.append((timestamp, mode, km1_value)) \n",
    "    return history\n",
    "\n",
    "def get_average_diff_from_matrix(matrix):\n",
    "    np_matrix = np.array(matrix)\n",
    "    mask = ~np.eye(np_matrix.shape[0], dtype=bool)\n",
    "    off = np_matrix[mask]\n",
    "    mean = (off.mean() if off.size else 0)\n",
    "    return mean\n",
    "\n",
    "def get_average_diff_from_matrices(matrices):\n",
    "    averages = []\n",
    "    for matrix in matrices:\n",
    "        avg = get_average_diff_from_matrix(matrix)\n",
    "        averages.append(avg)\n",
    "    return averages\n",
    "\n",
    "\n",
    "def get_max_diff_from_matrix(matrix):\n",
    "    np_matrix = np.array(matrix)\n",
    "    max_val = np_matrix.max()\n",
    "    return max_val\n",
    "\n",
    "def get_max_diff_from_matrices(matrices):\n",
    "    max_values = []\n",
    "    for matrix in matrices:\n",
    "        max_val = get_max_diff_from_matrix(matrix)\n",
    "        max_values.append(max_val)\n",
    "    return max_values\n",
    "\n",
    "# --- Figure saving support ---\n",
    "\n",
    "def _ensure_dir(path: str):\n",
    "    if path and not os.path.isdir(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def _finalize_figure(fig, show: bool, save: bool, save_dir: str, filename: str, dpi: int):\n",
    "    if save:\n",
    "        target_dir = save_dir or FIG_SAVE_DIR\n",
    "        _ensure_dir(target_dir)\n",
    "        out_path = os.path.join(target_dir, filename)\n",
    "        fig.savefig(out_path, dpi=dpi, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_combined_data(combined_data, title: str = \"Combined History and Difference Matrices\", filename: str = None,\n",
    "                       show: bool = True, save: bool = False, save_dir: str = None, dpi: int = 150):\n",
    "    \"\"\"Plot KM1 and diff metrics over time.\n",
    "    Params:\n",
    "      combined_data: list of dicts with 'timestamp','km1','diff_matrix'\n",
    "      filename: optional explicit filename (e.g. 'run1_combined.png')\n",
    "      show: display inline in notebook\n",
    "      save: if True, save image\n",
    "      save_dir: directory to save (defaults to FIG_SAVE_DIR)\n",
    "      dpi: resolution for saved figure\n",
    "    \"\"\"\n",
    "    timestamps = [entry['timestamp'] for entry in combined_data]\n",
    "    km1_values = [entry['km1'] for entry in combined_data]\n",
    "    avg_diffs = [get_average_diff_from_matrix(entry['diff_matrix']) for entry in combined_data]\n",
    "    max_diffs = [get_max_diff_from_matrix(entry['diff_matrix']) for entry in combined_data]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # First subplot: KM1 values\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax1.set_ylabel('KM1 Value', color=color, fontsize=12)\n",
    "    ax1.plot(timestamps, km1_values, color=color, marker='o', label='KM1 Value')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_title('KM1 Value over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Second subplot: Average and Max Differences\n",
    "    ax2.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax2.set_ylabel('Difference Value', fontsize=12)\n",
    "    ax2.plot(timestamps, avg_diffs, color='tab:red', marker='x', label='Average Difference')\n",
    "    ax2.plot(timestamps, max_diffs, color='tab:orange', marker='s', label='Max Difference')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_title('Difference Metrics over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename is None:\n",
    "        safe = title.lower().replace(' ', '_').replace('/', '_')\n",
    "        filename = f\"{safe}.png\"\n",
    "    _finalize_figure(fig, show, save, save_dir, filename, dpi)\n",
    "\n",
    "\n",
    "def combine_history_and_diff(history_run, diff_run, time_limit: float = None):\n",
    "    combined_data = []\n",
    "    index = 0\n",
    "    start_time = None\n",
    "    \n",
    "    for step in history_run:\n",
    "        timestamp, mode, km1_value = step\n",
    "        if mode == None:\n",
    "            start_time = timestamp\n",
    "            continue\n",
    "        if mode == 'Initial':\n",
    "            # skip\n",
    "            continue\n",
    "        \n",
    "        relative_time = (timestamp - start_time) / 1000.0  \n",
    "        \n",
    "        combined_data.append({\n",
    "            'timestamp': relative_time,\n",
    "            'mode': mode,\n",
    "            'km1': km1_value,\n",
    "            'diff_matrix': diff_run[index]\n",
    "        })\n",
    "        index += 1\n",
    "    ## Append last diff matrix if any left\n",
    "    if index < len(diff_run):\n",
    "        combined_data.append({\n",
    "            'timestamp': time_limit,\n",
    "            'mode': mode,\n",
    "            'km1': km1_value,\n",
    "            'diff_matrix': diff_run[-1]\n",
    "        })\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "def aggregate_history_runs(directory_path: str, full_history: bool = False, instances_to_exclude: list = []):\n",
    "    history_runs = {}\n",
    "    for file_path in glob.glob(directory_path + \"/*.csv\"):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        thread_id = file_name.split('.')[-3]\n",
    "        instance_name = '.'.join(file_name.split('.')[:-3]) \n",
    "        \n",
    "        use_fixed_seed = True\n",
    "        if full_history:\n",
    "            use_fixed_seed = False\n",
    "        instance_name = convert_instance_naming_scheme(instance_name, use_fixed_seed)\n",
    "        \n",
    "        purely_instance_name = '.'.join(instance_name.split('.')[:-4])\n",
    "        if purely_instance_name in instances_to_exclude:\n",
    "            continue\n",
    "        \n",
    "        history = parse_history_file(file_path)\n",
    "        \n",
    "        # Last entry should be the final result\n",
    "        km1 = None\n",
    "        for timestamp, mode, km1_value in reversed(history):\n",
    "            if mode is not None:\n",
    "                km1 = km1_value\n",
    "                break\n",
    "\n",
    "        # Store only the history for the best run\n",
    "        if instance_name not in history_runs:\n",
    "            history_runs[instance_name] = {'thread_id': thread_id, 'km1': km1, 'history': history}\n",
    "        else:\n",
    "            if history_runs[instance_name]['km1'] is None:\n",
    "                pass\n",
    "            if km1 is not None and km1 < history_runs[instance_name]['km1']:\n",
    "                history_runs[instance_name] = {'thread_id': thread_id, 'km1': km1, 'history': history}\n",
    "\n",
    "    return history_runs\n",
    "\n",
    "def get_diff_matrices_for_best_run(history_runs, diff_matrices_list, instance_name: str):\n",
    "    if instance_name not in history_runs:\n",
    "        return None\n",
    "\n",
    "    best_thread_id = history_runs[instance_name]['thread_id']\n",
    "    for entry in diff_matrices_list:\n",
    "        if entry['thread_id'] == best_thread_id:\n",
    "            return entry['matrices']\n",
    "    return None\n",
    "\n",
    "\n",
    "def aggregate_diff_runs(directory_path: str, instances_to_exclude: list = []):\n",
    "    diff_runs = {}\n",
    "    for file_path in glob.glob(directory_path + \"/*.csv\"):\n",
    "        matrices = bv.parse_diff_matrices(file_path)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        thread_id = file_name.split('.')[-3]\n",
    "        instance_name = '.'.join(file_name.split('.')[:-3])\n",
    "        use_fixed_seed = True\n",
    "        instance_name = convert_instance_naming_scheme(instance_name, use_fixed_seed)\n",
    "        purely_instance_name = '.'.join(instance_name.split('.')[:-4])\n",
    "        if purely_instance_name in instances_to_exclude:\n",
    "            continue\n",
    "        if instance_name not in diff_runs:\n",
    "            diff_runs[instance_name] = []    \n",
    "        diff_runs[instance_name].append({\n",
    "            'thread_id': thread_id,\n",
    "            'matrices': matrices\n",
    "        })\n",
    "\n",
    "    return diff_runs\n",
    "\n",
    "def split_long_short_history_runs(history_runs):\n",
    "    long_runs = {}\n",
    "    short_runs = {}\n",
    "    for instance_name, run_data in history_runs.items():\n",
    "        timelimit = int(instance_name.split('.')[-1])\n",
    "        if timelimit == SHORT_TIMELIMIT:\n",
    "            short_runs[instance_name] = run_data\n",
    "        else:\n",
    "            long_runs[instance_name] = run_data\n",
    "    return short_runs, long_runs\n",
    "\n",
    "def split_seed_history_runs(history_runs):\n",
    "    seed_runs = {}\n",
    "    for instance_name, run_data in history_runs.items():\n",
    "        seed = instance_name.split('.')[-2]\n",
    "        if seed not in seed_runs:\n",
    "            seed_runs[seed] = {}\n",
    "        seed_runs[seed][instance_name] = run_data\n",
    "        \n",
    "    ## Convert to list\n",
    "    seed_count = seed_runs.keys().__len__()\n",
    "    all_seeds_runs = [None] * seed_count\n",
    "    for seed, runs in seed_runs.items():\n",
    "        seed_int= int(seed)\n",
    "        all_seeds_runs[seed_int - 1] = runs\n",
    "    return all_seeds_runs\n",
    "\n",
    "def instance_to_seeds(history_runs):\n",
    "    # Dictionary maps instance_name to its seed runs\n",
    "    instance_to_seeds = {}\n",
    "    for instance_name, run_data in history_runs.items():\n",
    "        purely_instance_name = '.'.join(instance_name.split('.')[:-4])\n",
    "        seed = int(instance_name.split('.')[-2])\n",
    "        if purely_instance_name not in instance_to_seeds:\n",
    "            instance_to_seeds[purely_instance_name] = {}\n",
    "        instance_to_seeds[purely_instance_name][seed] = run_data\n",
    "    return instance_to_seeds\n",
    "\n",
    "def split_k_value_history_runs(history_runs, *k_values):\n",
    "    k_runs_list = [{} for _ in k_values]\n",
    "    k_value_to_index = {k: i for i, k in enumerate(k_values)}\n",
    "\n",
    "    for instance_name, run_data in history_runs.items():\n",
    "        k_from_instance = instance_name.split('.')[-3]\n",
    "        \n",
    "        if k_from_instance in k_value_to_index:\n",
    "            index = k_value_to_index[k_from_instance]\n",
    "            k_runs_list[index][instance_name] = run_data\n",
    "            \n",
    "    return k_runs_list\n",
    "\n",
    "def convert_time_for_history_run(history_run):   \n",
    "    converted_history = []\n",
    "    start_time = None\n",
    "    for timestamp, mode, km1_value in history_run:\n",
    "        if mode is None:\n",
    "            start_time = timestamp\n",
    "            # check if already converted first\n",
    "            if start_time == 0:\n",
    "                return history_run.copy()\n",
    "            converted_history.append((0, mode, km1_value))\n",
    "        else:\n",
    "            relative_time = (timestamp - start_time) / 1000.0  \n",
    "            converted_history.append((relative_time, mode, km1_value))\n",
    "    return converted_history\n",
    "\n",
    "\n",
    "def make_history_runs_sequential(*history_runs, time_limit=None):\n",
    "    combined_runs = {}\n",
    "    padded_history_runs = [None] * len(history_runs)\n",
    "    # Add time_limit to history run timestamps\n",
    "    for i, history_run in enumerate(history_runs):\n",
    "        padded_history_run = {}\n",
    "        for instance_name, run_data in history_run.items():\n",
    "            # Fix seed to 1 for combined run\n",
    "            instance_name_converted = instance_name.rsplit('.', 2)[0] + '.1.' + instance_name.rsplit('.', 1)[1]\n",
    "            \n",
    "            # Create a COPY of the history to avoid modifying original\n",
    "            history = [entry for entry in run_data['history']]  # Copy list\n",
    "            converted_history = convert_time_for_history_run(history)\n",
    "            \n",
    "            # Create new history with offset times\n",
    "            new_history = []\n",
    "            for timestamp, mode, km1_value in converted_history:\n",
    "                new_history.append((timestamp + i * time_limit, mode, km1_value))\n",
    "            \n",
    "            padded_history_run[instance_name_converted] = {\n",
    "                'thread_id': run_data['thread_id'], \n",
    "                'km1': run_data['km1'], \n",
    "                'history': new_history\n",
    "            }\n",
    "        padded_history_runs[i] = padded_history_run\n",
    "    # Combine all padded history runs\n",
    "    combined_runs = merge_histories(*padded_history_runs)\n",
    "    return combined_runs\n",
    "\n",
    "def merge_histories(*history_runs):\n",
    "    merged_histories = {}\n",
    "    for history_run in history_runs:\n",
    "        for instance_name, run_data in history_run.items():\n",
    "            if instance_name not in merged_histories:\n",
    "                # Deep copy the run_data\n",
    "                merged_histories[instance_name] = {\n",
    "                    'thread_id': run_data['thread_id'],\n",
    "                    'km1': run_data['km1'],\n",
    "                    'history': [entry for entry in run_data['history']]  # Copy history\n",
    "                }\n",
    "            else:\n",
    "                existing_history = merged_histories[instance_name]['history']\n",
    "                new_history = run_data['history']\n",
    "                # Append new history entries (creates new list)\n",
    "                merged_histories[instance_name]['history'] = existing_history + [entry for entry in new_history]\n",
    "                # Update km1 and thread_id if new run is better\n",
    "                if run_data['km1'] < merged_histories[instance_name]['km1']:\n",
    "                    merged_histories[instance_name]['km1'] = run_data['km1']\n",
    "                    merged_histories[instance_name]['thread_id'] = run_data['thread_id']\n",
    "                \n",
    "    # Sort histories by timestamp\n",
    "    for instance_name, run_data in merged_histories.items():\n",
    "        run_data['history'].sort(key=lambda x: x[0])\n",
    "    # History for each instance shall only contain decreasing km1 values\n",
    "    for instance_name, run_data in merged_histories.items():\n",
    "        filtered_history = []\n",
    "        last_km1 = float('inf')\n",
    "        for entry in run_data['history']:\n",
    "            time, mode, km1_value = entry\n",
    "            if km1_value is not None and km1_value < last_km1:\n",
    "                filtered_history.append(entry)\n",
    "                last_km1 = km1_value\n",
    "            elif time == 0:\n",
    "                filtered_history.append((0, None, None))\n",
    "        run_data['history'] = filtered_history\n",
    "    return merged_histories\n",
    "               \n",
    "def split_runs_k_value(runs, *k_values):\n",
    "# Create a list of empty dictionaries, one for each k-value\n",
    "    k_runs_list = [{} for _ in k_values]\n",
    "    \n",
    "    # Create a mapping from k_value to its index for quick lookups\n",
    "    k_value_to_index = {k: i for i, k in enumerate(k_values)}\n",
    "\n",
    "    for instance_name, run_data in runs.items():\n",
    "        k_from_instance = instance_name.split('.')[-2]\n",
    "        \n",
    "        if k_from_instance in k_value_to_index:\n",
    "            index = k_value_to_index[k_from_instance]\n",
    "            k_runs_list[index][instance_name] = run_data\n",
    "                \n",
    "    return k_runs_list\n",
    "\n",
    "def extract_info_from_config(config_name: str):\n",
    "    \n",
    "    result_dict = {}\n",
    "\n",
    "    # Remove leading date + underscore if present\n",
    "    base = re.sub(r'^\\d{4}-\\d{1,2}-\\d{1,2}_', '', config_name)\n",
    "\n",
    "    # Split different parameters on '-'\n",
    "    # Filter out empty strings (in case of accidental double '-')\n",
    "    param_groups = [g for g in base.split('-') if g]\n",
    "\n",
    "    for group in param_groups:\n",
    "        parts = group.split('_')\n",
    "        if not parts or parts[0] == '':\n",
    "            continue\n",
    "        param_name = parts[0]\n",
    "        values = parts[1:] if len(parts) > 1 else []\n",
    "        # Only store if we have at least one value; keep empty list otherwise\n",
    "        result_dict[param_name] = values\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def create_mean_over_seeds_single_instance(seeds_to_run, instance_name: str):  \n",
    "    result_list = []\n",
    "    current_indices = [0] * len(seeds_to_run)\n",
    "    current_values = [None] * len(seeds_to_run)\n",
    "    current_times = [0] * len(seeds_to_run)\n",
    "    max_time = 0\n",
    "    \n",
    "    converted_runs = []\n",
    "    for seed_run in seeds_to_run.values():\n",
    "        converted_history = convert_time_for_history_run(seed_run['history'])\n",
    "        converted_runs.append(converted_history)\n",
    "    \n",
    "    # First value is special case\n",
    "    for i, history in enumerate(converted_runs):\n",
    "        time, mode, km1_value = history[1]\n",
    "        current_values[i] = km1_value\n",
    "        current_indices[i] = 1\n",
    "        current_times[i] = time\n",
    "        if time > max_time:\n",
    "            max_time = time\n",
    "    # update other instances to match max_time\n",
    "    for i in range(len(current_times)):\n",
    "        while current_times[i] < max_time:\n",
    "            index = current_indices[i]\n",
    "            history = converted_runs[i]\n",
    "            if index + 1 < len(history) and history[index + 1][0] <= max_time:\n",
    "                time, mode, km1_value = history[index + 1]\n",
    "                current_indices[i] += 1\n",
    "                current_values[i] = km1_value\n",
    "                current_times[i] = time\n",
    "            else:\n",
    "                break\n",
    "    result_list.append((max_time, mean(current_values)))\n",
    "    \n",
    "    end = False\n",
    "    while not end:\n",
    "        next_time = float('inf')\n",
    "        for i, history in enumerate(converted_runs):\n",
    "            index = current_indices[i]\n",
    "            if index + 1 < len(history):\n",
    "                time, mode, km1_value = history[index + 1]\n",
    "                if time < next_time:\n",
    "                    next_time = time\n",
    "                    instance_to_increment = i\n",
    "                    km1_to_update = km1_value\n",
    "        ## next time found\n",
    "        if next_time == float('inf'):\n",
    "            end = True\n",
    "            continue\n",
    "        # Increment the current index for the instance with the next time\n",
    "        current_indices[instance_to_increment] += 1\n",
    "        current_values[instance_to_increment] = km1_to_update\n",
    "        current_times[instance_to_increment] = next_time\n",
    "        \n",
    "        # append result list\n",
    "        result_list.append((next_time, mean(current_values)))\n",
    "    return result_list\n",
    "        \n",
    "        \n",
    "def max_iterations_per_instance(iteration_runs, multiple_seeds: bool = False):\n",
    "    # Return the maximum iterations per instance (avg betwen multiple seeds) as a dict\n",
    "    max_iterations = {}\n",
    "    if multiple_seeds:\n",
    "        instance_to_seeds_dict = instance_to_seeds(iteration_runs)\n",
    "        for instance_name, seed_runs in instance_to_seeds_dict.items():\n",
    "            max_iters_per_seed = {}\n",
    "            for seed_run in seed_runs.values():\n",
    "                timestamp_to_iteration, iteration_to_metric_value = seed_run\n",
    "                # Get the last iteration\n",
    "                last_iteration = iteration_to_metric_value[-1][0]\n",
    "                if instance_name not in max_iters_per_seed:\n",
    "                    max_iters_per_seed[instance_name] = []\n",
    "                max_iters_per_seed[instance_name].append(last_iteration)\n",
    "            max_iterations[instance_name] = mean(max_iters_per_seed[instance_name])\n",
    "    else:\n",
    "        for instance_name, run_data in iteration_runs.items():\n",
    "            timestamp_to_iteration, iteration_to_metric_value = run_data\n",
    "            last_iteration = iteration_to_metric_value[-1][0]\n",
    "            max_iterations[instance_name] = last_iteration\n",
    "    \n",
    "    return max_iterations\n",
    "    \n",
    "def get_iteration_multipliers(baseline_max_iterations, other_max_iterations):\n",
    "    multipliers = {}\n",
    "    for instance_name in baseline_max_iterations:\n",
    "        if instance_name in other_max_iterations:\n",
    "            baseline_iters = baseline_max_iterations[instance_name]\n",
    "            other_iters = other_max_iterations[instance_name]\n",
    "            if baseline_iters > 0:\n",
    "                multiplier = other_iters / baseline_iters\n",
    "                multipliers[instance_name] = multiplier\n",
    "    if multipliers:\n",
    "        return multipliers\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def create_multiplier_based_history_runs(history_runs, multipliers):\n",
    "    adjusted_history_runs = {}\n",
    "    for instance_name, run_data in history_runs.items():\n",
    "        # Get first timestamp\n",
    "        last_initial_timestamp = run_data['history'][0][0] if run_data['history'] else 0\n",
    "        purely_instance_name = '.'.join(instance_name.split('.')[:-4])\n",
    "        if purely_instance_name in multipliers:\n",
    "            multiplier = multipliers[purely_instance_name]\n",
    "            adjusted_history = []\n",
    "            for timestamp, mode, km1_value in run_data['history']:\n",
    "                time_diff = timestamp - last_initial_timestamp\n",
    "                if mode is not None and mode != 'Initial':\n",
    "                    # Round to nearest integer\n",
    "                    adjusted_time = last_initial_timestamp + round(time_diff * multiplier)\n",
    "                else:\n",
    "                    last_initial_timestamp = timestamp\n",
    "                    adjusted_time = timestamp\n",
    "                adjusted_history.append((adjusted_time, mode, km1_value))\n",
    "            adjusted_history_runs[instance_name] = {\n",
    "                'thread_id': run_data['thread_id'],\n",
    "                'km1': run_data['km1'],\n",
    "                'history': adjusted_history\n",
    "            }\n",
    "        else:\n",
    "            # No adjustment if no multiplier found\n",
    "            adjusted_history_runs[instance_name] = run_data\n",
    "    return adjusted_history_runs\n",
    "           \n",
    "                    \n",
    "def create_geomean_over_all_instances(history_runs, multiple_seeds: bool = False):\n",
    "    result_list = []\n",
    "    current_indices = {}\n",
    "    current_values = {}\n",
    "    current_times = {}\n",
    "    max_time = 0\n",
    "    \n",
    "    if multiple_seeds:\n",
    "        instance_to_seeds_dict = instance_to_seeds(history_runs)\n",
    "        # For each instance, compute mean over seeds\n",
    "        averaged_history_runs = {}\n",
    "        for instance_name, seed_runs in instance_to_seeds_dict.items():\n",
    "            mean_history = create_mean_over_seeds_single_instance(seed_runs, instance_name)\n",
    "            # history list of tuples (time, mode, km1)\n",
    "            history = [(time, None, km1) for time, km1 in mean_history]\n",
    "            averaged_history_runs[instance_name] = history\n",
    "        converted_runs = averaged_history_runs\n",
    "    \n",
    "    # Skip conversion if already done for multiple seeds\n",
    "    if not multiple_seeds:\n",
    "        converted_runs = {}\n",
    "        for instance_name, run_data in history_runs.items():\n",
    "            converted_history = convert_time_for_history_run(run_data['history'])\n",
    "            converted_runs[instance_name] = converted_history\n",
    "            \n",
    "        \n",
    "    # First value is special case\n",
    "    for i, (instance_name, history) in enumerate(converted_runs.items()):\n",
    "        time, mode, km1_value = history[0]\n",
    "        current_values[instance_name] = km1_value\n",
    "        current_indices[instance_name] = 1\n",
    "        current_times[instance_name] = time\n",
    "        if time > max_time:\n",
    "            max_time = time\n",
    "    \n",
    "    # update other instances to match max_time\n",
    "    for instance_name in current_times:\n",
    "        while current_times[instance_name] < max_time:\n",
    "            index = current_indices[instance_name]\n",
    "            history = converted_runs[instance_name]\n",
    "            if index + 1 < len(history) and history[index + 1][0] <= max_time:\n",
    "                time, mode, km1_value = history[index + 1]\n",
    "                current_indices[instance_name] += 1\n",
    "                current_values[instance_name] = km1_value\n",
    "                current_times[instance_name] = time\n",
    "            else:\n",
    "                break\n",
    "    # Check for 0 values in current_results and substitute with 1\n",
    "    for instance_name in current_values:\n",
    "        if current_values[instance_name] == 0:\n",
    "            current_values[instance_name] = 1 \n",
    "    result_list.append((max_time, geometric_mean(current_values.values())))\n",
    "    \n",
    "    end = False\n",
    "    while not end:\n",
    "        next_time = float('inf')\n",
    "        for i, (instance_name, history) in enumerate(converted_runs.items()):\n",
    "            index = current_indices[instance_name]\n",
    "            if index + 1 < len(history):\n",
    "                time, mode, km1_value = history[index + 1]\n",
    "                if time < next_time:\n",
    "                    next_time = time\n",
    "                    instance_to_increment = instance_name\n",
    "                    km1_to_update = km1_value\n",
    "        ## next time found\n",
    "        if next_time == float('inf'):\n",
    "            end = True\n",
    "            continue\n",
    "        # Increment the current index for the instance with the next time\n",
    "        current_indices[instance_to_increment] += 1\n",
    "        current_values[instance_to_increment] = km1_to_update\n",
    "        current_times[instance_to_increment] = next_time\n",
    "        \n",
    "        # append result list\n",
    "        result_list.append((next_time, geometric_mean(current_values.values())))\n",
    "\n",
    "    return result_list\n",
    "\n",
    "def eval_k_kway(\n",
    "    configs_name=\"\",\n",
    "    all_configs_root=os.path.expanduser(\"~/Documents/experiment_results\"),\n",
    "    results_dirname=\"mt_kahypar_evo_results\",\n",
    "    diff_dirname=\"evo_diff\",\n",
    "    history_dirname=\"evo_history\",\n",
    "    instances_to_exclude=None,\n",
    "    show_full_history=False,\n",
    "    default_pop_size=10,\n",
    "):\n",
    "\n",
    "    all_configs_dir = os.path.join(all_configs_root, configs_name)\n",
    "\n",
    "    # Loop through each folder inside DIR\n",
    "    all_geomeans = {}\n",
    "\n",
    "    for config_dir in glob.glob(all_configs_dir + \"/*/\"):\n",
    "        config_name = os.path.basename(os.path.normpath(config_dir))\n",
    "        print(f\"Processing configuration: {config_name}\")\n",
    "\n",
    "        config_info = extract_info_from_config(config_name)\n",
    "\n",
    "        k_values = config_info.get('k', None)\n",
    "        kway_value = config_info.get('kway', None)\n",
    "        if kway_value:\n",
    "            kway_value = kway_value[0]\n",
    "        pop_size = config_info.get('pop', None)\n",
    "\n",
    "        result_dir = os.path.join(config_dir, results_dirname)\n",
    "        diff_dir = os.path.join(config_dir, diff_dirname)\n",
    "        history_dir = os.path.join(config_dir, history_dirname)\n",
    "\n",
    "        # Aggregate runs from result files\n",
    "        runs = aggregate_runs(result_dir, instances_to_exclude=instances_to_exclude)\n",
    "        k_runs = split_runs_k_value(runs, *k_values) if k_values else [runs]\n",
    "        k_runs_dict = {}\n",
    "        if k_values:\n",
    "            for i, k in enumerate(k_values):\n",
    "                k_runs_dict[k] = k_runs[i]\n",
    "\n",
    "        history_runs = aggregate_history_runs(history_dir, full_history=show_full_history, instances_to_exclude=instances_to_exclude)\n",
    "        #diff_runs = aggregate_diff_runs(diff_dir, instances_to_exclude=instances_to_exclude)  # kept for completeness\n",
    "        k_history_runs = split_k_value_history_runs(history_runs, *k_values) if k_values else [history_runs]\n",
    "        k_history_runs_dict = {}\n",
    "        if k_values:\n",
    "            for i, k in enumerate(k_values):\n",
    "                k_history_runs_dict[k] = k_history_runs[i]\n",
    "\n",
    "        # Create Geomeans for each configuration\n",
    "        for k in k_values:\n",
    "            k_run = k_runs_dict[k]\n",
    "            k_history_run = k_history_runs_dict[k]\n",
    "            if k == '64' and kway_value == '2':\n",
    "                pass\n",
    "            geomean = create_geomean_over_all_instances(k_history_run, multiple_seeds=show_full_history)\n",
    "            config = (k, kway_value, pop_size[0] if pop_size else default_pop_size)\n",
    "            all_geomeans[config] = geomean\n",
    "\n",
    "    # Plot all kway geomeans for each k-value in separate plots (data prep only)\n",
    "    k_pop_to_kway_geomeans = {}\n",
    "    for (k, kway, pop_size), geomean in all_geomeans.items():\n",
    "        key = (k, pop_size)\n",
    "        if key not in k_pop_to_kway_geomeans:\n",
    "            k_pop_to_kway_geomeans[key] = {}\n",
    "        k_pop_to_kway_geomeans[key][kway] = geomean\n",
    "\n",
    "    # Directory where plots will be saved by the final loop\n",
    "    save_path = os.path.join(FIG_SAVE_DIR, configs_name)\n",
    "    return k_pop_to_kway_geomeans, save_path\n",
    "\n",
    "def eval_evothreads(\n",
    "    configs_name=\"\",\n",
    "    all_configs_root=os.path.expanduser(\"~/Documents/experiment_results\"),\n",
    "    results_dirname=\"mt_kahypar_evo_results\",\n",
    "    diff_dirname=\"evo_diff\",\n",
    "    history_dirname=\"evo_history\",\n",
    "    instances_to_exclude=None,\n",
    "    show_full_history=False\n",
    "):\n",
    "    all_configs_dir = os.path.join(all_configs_root, configs_name)\n",
    "    # Loop through each folder inside DIR\n",
    "    thread_count = 1\n",
    "    all_geomeans = {}\n",
    "    for config_dir in glob.glob(all_configs_dir + \"/*/\"):\n",
    "        config_name = os.path.basename(os.path.normpath(config_dir))\n",
    "        print(f\"Processing configuration: {config_name}\")\n",
    "        \n",
    "        config_info = extract_info_from_config(config_name)\n",
    "        threads_per_worker = config_info.get('parallel', None)\n",
    "        \n",
    "        result_dir = os.path.join(config_dir, results_dirname)\n",
    "        diff_dir = os.path.join(config_dir, diff_dirname)\n",
    "        history_dir = os.path.join(config_dir, history_dirname)\n",
    "        \n",
    "        # Aggregate runs from result files\n",
    "        runs = aggregate_runs(result_dir, instances_to_exclude=instances_to_exclude)\n",
    "        history_runs = aggregate_history_runs(history_dir, full_history=show_full_history, instances_to_exclude=instances_to_exclude)\n",
    "\n",
    "        if not show_full_history:\n",
    "            geomean = create_geomean_over_all_instances(history_runs)\n",
    "        else:\n",
    "            geomean = create_geomean_over_all_instances(history_runs, multiple_seeds=True)\n",
    "        threads_per_worker = int(threads_per_worker[0]) if threads_per_worker else \"default\"\n",
    "        thread_count = max(thread_count, threads_per_worker)\n",
    "        all_geomeans[threads_per_worker] = geomean\n",
    "        \n",
    "    # Directory where plots will be saved by the final loop\n",
    "    save_path = os.path.join(FIG_SAVE_DIR, configs_name)\n",
    "    return all_geomeans, save_path, thread_count\n",
    "    \n",
    "    \n",
    "def eval_generic_by_name(\n",
    "    configs_name=\"\",\n",
    "    all_configs_root=os.path.expanduser(\"~/Documents/experiment_results\"),\n",
    "    results_dirname=\"mt_kahypar_evo_results\",\n",
    "    diff_dirname=\"evo_diff\",\n",
    "    history_dirname=\"evo_history\",\n",
    "    instances_to_exclude=None,\n",
    "    show_full_history=False,\n",
    "    plot_iterations=False\n",
    "): \n",
    "    all_configs_dir = os.path.join(all_configs_root, configs_name)\n",
    "    # Loop through each folder inside DIR\n",
    "    all_geomeans = {}\n",
    "    for config_dir in glob.glob(all_configs_dir + \"/*/\"):\n",
    "        config_name = os.path.basename(os.path.normpath(config_dir))\n",
    "        print(f\"Processing configuration: {config_name}\")\n",
    "        \n",
    "        result_dir = os.path.join(config_dir, results_dirname)\n",
    "        diff_dir = os.path.join(config_dir, diff_dirname)\n",
    "        history_dir = os.path.join(config_dir, history_dirname)\n",
    "        \n",
    "        # Aggregate runs from result files\n",
    "        runs = aggregate_runs(result_dir, instances_to_exclude=instances_to_exclude)\n",
    "        history_runs = aggregate_history_runs(history_dir, full_history=show_full_history, instances_to_exclude=instances_to_exclude)\n",
    "\n",
    "        if not show_full_history:\n",
    "            geomean = create_geomean_over_all_instances(history_runs)\n",
    "        else:\n",
    "            geomean = create_geomean_over_all_instances(history_runs, multiple_seeds=True)\n",
    "        \n",
    "        # Remove date prefix if present    \n",
    "        config_name_cleaned = remove_date_prefix(config_name)\n",
    "        all_geomeans[config_name_cleaned] = geomean\n",
    "        \n",
    "    # Directory where plots will be saved by the final loop\n",
    "    save_path = os.path.join(FIG_SAVE_DIR, configs_name)\n",
    "    return all_geomeans, save_path\n",
    "    \n",
    "    \n",
    "def remove_date_prefix(name: str) -> str:\n",
    "    return re.sub(r'^\\d{4}-\\d{1,2}-\\d{1,2}_', '', name)\n",
    "    \n",
    "def aggregate_iterations_runs(directory_path: str, show_full_history: bool = False, instances_to_exclude: list = []):\n",
    "    iteration_runs = {}\n",
    "    for file_path in glob.glob(directory_path + \"/*.csv\"):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        instance_name = '.'.join(file_name.split('.')[:-3]) \n",
    "        \n",
    "        use_fixed_seed = True\n",
    "        if show_full_history:\n",
    "            use_fixed_seed = False\n",
    "        instance_name = convert_instance_naming_scheme(instance_name, use_fixed_seed)\n",
    "        \n",
    "        purely_instance_name = '.'.join(instance_name.split('.')[:-4])\n",
    "        if purely_instance_name in instances_to_exclude:\n",
    "            continue\n",
    "        \n",
    "        timestamp_to_iteration, iteration_to_metric_value = get_iteration_run(file_path)\n",
    "        iteration_runs[instance_name] = (timestamp_to_iteration, iteration_to_metric_value)\n",
    "    return iteration_runs\n",
    "\n",
    "def get_iteration_run(iteration_run_file: str):\n",
    "    ## Return two lists: timestamp_to_iteration and iteration_to_metric_value\n",
    "    timestamp_to_iteration = []\n",
    "    iteration_to_metric_value = []\n",
    "    # iteration log file format: iteration, timestamp, metric_value\n",
    "    with open(iteration_run_file, 'r') as f:\n",
    "        first_timestamp = None\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) >= 3:\n",
    "                try:\n",
    "                    iteration = int(parts[0].strip())\n",
    "                    timestamp = int(parts[1].strip())\n",
    "                    metric_value = float(parts[2].strip())\n",
    "                    \n",
    "                    if first_timestamp is None:\n",
    "                        first_timestamp = timestamp\n",
    "                        \n",
    "                    normalized_timestamp = (timestamp - first_timestamp) / 1000.0  # in seconds\n",
    "                    \n",
    "                    timestamp_to_iteration.append((normalized_timestamp, iteration))\n",
    "                    iteration_to_metric_value.append((iteration, metric_value))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return timestamp_to_iteration, iteration_to_metric_value\n",
    "\n",
    "def simulate_stop_after_k_no_improvement_iterations(\n",
    "    timestamp_to_iteration, iteration_to_km1, k: int\n",
    "):\n",
    "    if not iteration_to_km1:\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    iter_to_time = {iter_: t for t, iter_ in timestamp_to_iteration}\n",
    "\n",
    "    first_iter, first_km1 = iteration_to_km1[0]\n",
    "    last_iter, last_km1 = iteration_to_km1[-1]\n",
    "    total_improvement = first_km1 - last_km1\n",
    "    start_time = iter_to_time.get(first_iter, 0.0)\n",
    "    end_time = iter_to_time.get(last_iter, start_time)\n",
    "    total_time = max(end_time - start_time, 1e-9)\n",
    "\n",
    "    best_so_far = first_km1\n",
    "    no_improve_count = 0\n",
    "\n",
    "    stop_iter = last_iter\n",
    "    stop_km1 = last_km1\n",
    "    stop_time = end_time\n",
    "\n",
    "    for it, km1 in iteration_to_km1[1:]:\n",
    "        if km1 < best_so_far:\n",
    "            best_so_far = km1\n",
    "            no_improve_count = 0\n",
    "            stop_iter = it\n",
    "            stop_km1 = km1\n",
    "            stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            if no_improve_count >= k:\n",
    "                stop_iter = it\n",
    "                stop_km1 = km1\n",
    "                stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "                break\n",
    "\n",
    "    improvement_at_stop = first_km1 - stop_km1\n",
    "    frac_improvement = (\n",
    "        improvement_at_stop / total_improvement if total_improvement > 0 else 1.0\n",
    "    )\n",
    "    frac_time = (stop_time - start_time) / total_time\n",
    "\n",
    "    return stop_time, stop_iter, stop_km1, last_km1, frac_improvement, frac_time\n",
    "\n",
    "def extract_improvements_from_iterations(iteration_to_km1):\n",
    "    if not iteration_to_km1:\n",
    "        return []\n",
    "\n",
    "    improvements = []\n",
    "    best_so_far = iteration_to_km1[0][1]\n",
    "    last_improv_iter = iteration_to_km1[0][0]\n",
    "\n",
    "    for it, km1 in iteration_to_km1[1:]:\n",
    "        if km1 < best_so_far:\n",
    "            delta = best_so_far - km1\n",
    "            improvements.append((it, km1, delta))\n",
    "            best_so_far = km1\n",
    "            last_improv_iter = it\n",
    "\n",
    "    # Contains tuples of (iteration, km1, delta) for each improvement\n",
    "    return improvements\n",
    "\n",
    "\n",
    "def _time_at_or_before_iteration(timestamp_to_iteration, target_it):\n",
    "    \"\"\"\n",
    "    Given a list of (time, iteration) pairs sorted by time,\n",
    "    return the time of the largest iteration <= target_it.\n",
    "    If all iterations are > target_it, return the first time.\n",
    "    \"\"\"\n",
    "    # timestamp_to_iteration: [(time0, it0), (time1, it1), ...]\n",
    "    last_time = timestamp_to_iteration[0][0]\n",
    "    last_it = timestamp_to_iteration[0][1]\n",
    "\n",
    "    for t, it in timestamp_to_iteration:\n",
    "        if it > target_it:\n",
    "            break\n",
    "        last_time = t\n",
    "        last_it = it\n",
    "\n",
    "    return last_time\n",
    "\n",
    "def simulate_stop_by_improvement_rate(\n",
    "    timestamp_to_iteration,\n",
    "    iteration_to_km1,\n",
    "    early_window_improvs: int = 5,\n",
    "    recent_window_improvs: int = 5,\n",
    "    alpha: float = 0.1,\n",
    "    max_iters_without_improv: int = 500,\n",
    "):\n",
    "    if len(iteration_to_km1) < 2:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    iter_to_time = {iter_: t for t, iter_ in timestamp_to_iteration}\n",
    "    first_iter, first_km1 = iteration_to_km1[0]\n",
    "    last_iter, last_km1 = iteration_to_km1[-1]\n",
    "    total_improvement = first_km1 - last_km1\n",
    "    start_time = iter_to_time.get(first_iter, 0.0)\n",
    "    end_time = iter_to_time.get(last_iter, start_time)\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    def km1_at_or_before(target_it):\n",
    "        for it, km1 in reversed(iteration_to_km1):\n",
    "            if it <= target_it:\n",
    "                return km1\n",
    "        return iteration_to_km1[0][1]\n",
    "    \n",
    "    # Extract improvements as (iter, km1, delta)\n",
    "    improvements = extract_improvements_from_iterations(iteration_to_km1)\n",
    "\n",
    "    # Fallback: handle too few improvements case (last k iterations without improvement)\n",
    "    if len(improvements) < early_window_improvs + 1:\n",
    "        best_so_far = first_km1\n",
    "        last_improv_iter = first_iter\n",
    "        stop_iter = last_iter\n",
    "        stop_km1 = last_km1\n",
    "        stop_time = end_time\n",
    "\n",
    "        for it, km1 in iteration_to_km1[1:]:\n",
    "            if km1 < best_so_far:\n",
    "                best_so_far = km1\n",
    "                last_improv_iter = it\n",
    "                stop_iter = it\n",
    "                stop_km1 = km1\n",
    "                stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "            else:\n",
    "                # Plateau fallback\n",
    "                if it - last_improv_iter >= max_iters_without_improv:\n",
    "                    stop_iter = it\n",
    "                    stop_km1 = best_so_far\n",
    "                    stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "                    break\n",
    "\n",
    "        improvement_at_stop = first_km1 - stop_km1\n",
    "        frac_improvement = (\n",
    "            improvement_at_stop / total_improvement if total_improvement > 0 else 1.0\n",
    "        )\n",
    "        frac_time = (stop_time - start_time) / total_time\n",
    "        return stop_time, stop_iter, stop_km1, last_km1, frac_improvement, frac_time\n",
    "\n",
    "\n",
    "    # Normal case\n",
    "\n",
    "    # Early rate from first early_window_improvs improvements\n",
    "    early_slice = improvements[:early_window_improvs]\n",
    "    it0, km1_0, _ = early_slice[0]\n",
    "    itW, km1_W, _ = early_slice[-1]\n",
    "    early_delta = km1_0 - km1_W\n",
    "    early_span = max(itW - it0, 1)\n",
    "    early_rate = early_delta / early_span\n",
    "\n",
    "    stop_iter = last_iter\n",
    "    stop_km1 = last_km1\n",
    "    stop_time = end_time\n",
    "\n",
    "    last_improv_iter = improvements[early_window_improvs - 1][0]\n",
    "\n",
    "    # Walk over improvements and compute recent rate over a sliding window of improvements\n",
    "    for i in range(early_window_improvs, len(improvements)):\n",
    "        window = improvements[max(0, i - recent_window_improvs + 1): i + 1]\n",
    "        it_start, _, _ = window[0]\n",
    "        it_end, km_end, _ = window[-1]\n",
    "        delta_sum = sum(d for _, _, d in window)\n",
    "        span = max(it_end - it_start, 1)\n",
    "        recent_rate = delta_sum / span\n",
    "\n",
    "        # Update last_improv_iter (most recent improvement considered)\n",
    "        last_improv_iter = it_end\n",
    "        \n",
    "        if recent_rate < alpha * early_rate:\n",
    "            stop_iter = it_end\n",
    "            stop_km1 = km_end\n",
    "            stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "            break\n",
    "\n",
    "        # Plateau fallback: check for distance to next improvement\n",
    "        if i + 1 < len(improvements):\n",
    "            next_improv_iter, _, _ = improvements[i + 1]\n",
    "            gap = next_improv_iter - last_improv_iter\n",
    "\n",
    "            # If the next improvement is further than max_iters_without_improv away then stop\n",
    "            if gap > max_iters_without_improv:\n",
    "                target_it = last_improv_iter + max_iters_without_improv\n",
    "                stop_iter = target_it\n",
    "                stop_km1 = km1_at_or_before(target_it)\n",
    "                stop_time = _time_at_or_before_iteration(timestamp_to_iteration, target_it)\n",
    "                improvement_at_stop = first_km1 - stop_km1\n",
    "                frac_improvement = (\n",
    "                    improvement_at_stop / total_improvement if total_improvement > 0 else 1.0\n",
    "                )\n",
    "                frac_time = (stop_time - start_time) / total_time\n",
    "                return (\n",
    "                    stop_time,\n",
    "                    stop_iter,\n",
    "                    stop_km1,\n",
    "                    last_km1,\n",
    "                    frac_improvement,\n",
    "                    frac_time,\n",
    "                )\n",
    "        else:\n",
    "            # No future improvements at all\n",
    "            target_it = min(last_improv_iter + max_iters_without_improv, last_iter)\n",
    "            stop_iter = target_it\n",
    "            stop_km1 = km1_at_or_before(target_it)\n",
    "            stop_time = _time_at_or_before_iteration(timestamp_to_iteration, target_it)\n",
    "            improvement_at_stop = first_km1 - stop_km1\n",
    "            frac_improvement = (\n",
    "                improvement_at_stop / total_improvement if total_improvement > 0 else 1.0\n",
    "            )\n",
    "            frac_time = (stop_time - start_time) / total_time\n",
    "            return (\n",
    "                stop_time,\n",
    "                stop_iter,\n",
    "                stop_km1,\n",
    "                last_km1,\n",
    "                frac_improvement,\n",
    "                frac_time,\n",
    "            )\n",
    "\n",
    "    improvement_at_stop = first_km1 - stop_km1\n",
    "    frac_improvement = (\n",
    "        improvement_at_stop / total_improvement if total_improvement > 0 else 1.0\n",
    "    )\n",
    "    frac_time = (stop_time - start_time) / total_time\n",
    "    return stop_time, stop_iter, stop_km1, last_km1, frac_improvement, frac_time\n",
    "\n",
    "\n",
    "\n",
    "def simulate_stop_by_global_tangent(\n",
    "    timestamp_to_iteration,\n",
    "    iteration_to_km1,\n",
    "    alpha: float = 0.1,\n",
    "    min_iters: int = 50,\n",
    "    max_iters_without_improv: int = 500,\n",
    "):\n",
    "    if len(iteration_to_km1) < 2:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    iter_to_time = {it: t for t, iter_ in timestamp_to_iteration}\n",
    "    first_iter, first_km1 = iteration_to_km1[0]\n",
    "    last_iter, last_km1 = iteration_to_km1[-1]\n",
    "\n",
    "    total_improvement = first_km1 - last_km1\n",
    "    start_time = iter_to_time.get(first_iter, 0.0)\n",
    "    end_time = iter_to_time.get(last_iter, start_time)\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # --- Find first improvement over the initial value ---\n",
    "    base_km1 = first_km1\n",
    "    first_improv_iter = None\n",
    "    first_improv_km1 = None\n",
    "\n",
    "    no_improv_since_start = 0\n",
    "\n",
    "    for it, km1 in iteration_to_km1[1:]:\n",
    "        if km1 < base_km1:\n",
    "            first_improv_iter = it\n",
    "            first_improv_km1 = km1\n",
    "            break\n",
    "        else:\n",
    "            no_improv_since_start += 1\n",
    "            if no_improv_since_start >= max_iters_without_improv:\n",
    "                # Plateau or optimal from the very beginning\n",
    "                stop_iter = it\n",
    "                stop_km1 = km1\n",
    "                stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "                improvement_at_stop = first_km1 - stop_km1\n",
    "                frac_improvement = (\n",
    "                    improvement_at_stop / total_improvement\n",
    "                    if total_improvement > 0\n",
    "                    else 1.0\n",
    "                )\n",
    "                frac_time = (stop_time - start_time) / total_time\n",
    "                return (\n",
    "                    stop_time,\n",
    "                    stop_iter,\n",
    "                    stop_km1,\n",
    "                    last_km1,\n",
    "                    frac_improvement,\n",
    "                    frac_time,\n",
    "                )\n",
    "\n",
    "    # No improvement at all before the end of the log\n",
    "    if first_improv_iter is None:\n",
    "        return end_time, last_iter, last_km1, last_km1, 1.0, 1.0\n",
    "\n",
    "    # helper to get best-so-far at or before an iteration\n",
    "    # only necessary becaue of \"skipped\" iterations in the log\n",
    "    def best_km1_at_iter(target_it):\n",
    "        for it, km1 in reversed(iteration_to_km1):\n",
    "            if it <= target_it:\n",
    "                return km1\n",
    "        return iteration_to_km1[0][1]\n",
    "\n",
    "    # Initial global rate is measured once we have enough iterations since first_improv_iter\n",
    "    init_rate_it = None\n",
    "    init_rate_val = None\n",
    "\n",
    "    for it, km1 in iteration_to_km1:\n",
    "        if it < first_improv_iter + min_iters:\n",
    "            continue\n",
    "        curr_best = best_km1_at_iter(it)\n",
    "        delta = first_improv_km1 - curr_best\n",
    "        span = max(it - first_improv_iter, 1)\n",
    "        rate = delta / span\n",
    "        init_rate_it = it\n",
    "        init_rate_val = rate\n",
    "        break\n",
    "\n",
    "    # If we never got enough iterations, just run to the end\n",
    "    if init_rate_it is None or init_rate_val <= 0:\n",
    "        return end_time, last_iter, last_km1, last_km1, 1.0, 1.0\n",
    "\n",
    "    best_so_far = best_km1_at_iter(init_rate_it)\n",
    "    last_improv_iter = init_rate_it\n",
    "    stop_iter = last_iter\n",
    "    stop_km1 = last_km1\n",
    "    stop_time = end_time\n",
    "\n",
    "    for it, km1 in iteration_to_km1:\n",
    "        if it < init_rate_it:\n",
    "            continue\n",
    "\n",
    "        if km1 < best_so_far:\n",
    "            best_so_far = km1\n",
    "            last_improv_iter = it\n",
    "\n",
    "        #Check idle plateau as a backup (local minimum / true optimum)\n",
    "        if it - last_improv_iter >= max_iters_without_improv:\n",
    "            stop_iter = it\n",
    "            stop_km1 = best_so_far\n",
    "            stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "            break\n",
    "\n",
    "        # Compute global rate from first_improv_iter to current it\n",
    "        curr_best = best_so_far\n",
    "        delta = first_improv_km1 - curr_best\n",
    "        span = max(it - first_improv_iter, 1)\n",
    "        curr_rate = delta / span\n",
    "\n",
    "        # Compare to initial global rate\n",
    "        if curr_rate <= alpha * init_rate_val:\n",
    "            stop_iter = it\n",
    "            stop_km1 = curr_best\n",
    "            stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "            break\n",
    "\n",
    "    improvement_at_stop = first_km1 - stop_km1\n",
    "    frac_improvement = (\n",
    "        improvement_at_stop / total_improvement if total_improvement > 0 else 1.0\n",
    "    )\n",
    "    frac_time = (stop_time - start_time) / total_time\n",
    "\n",
    "    return stop_time, stop_iter, stop_km1, last_km1, frac_improvement, frac_time\n",
    "\n",
    "\n",
    "def simulate_stop_by_improvement_rate_iter_window(\n",
    "    timestamp_to_iteration,\n",
    "    iteration_to_km1,\n",
    "    early_window_iters: int = 100,\n",
    "    recent_window_iters: int = 20,\n",
    "    alpha: float = 0.1,\n",
    "    max_iters_without_improv: int = 500\n",
    "):\n",
    "    if len(iteration_to_km1) < 2:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    iter_to_time = {iter_: t for t, iter_ in timestamp_to_iteration}\n",
    "    first_iter, first_km1 = iteration_to_km1[0]\n",
    "    last_iter, last_km1 = iteration_to_km1[-1]\n",
    "    total_improvement = first_km1 - last_km1\n",
    "    start_time = iter_to_time.get(first_iter, 0.0)\n",
    "    end_time = iter_to_time.get(last_iter, start_time)\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    def km1_at_iter(target_it):\n",
    "        for it, km1 in reversed(iteration_to_km1):\n",
    "            if it <= target_it:\n",
    "                return km1\n",
    "        return iteration_to_km1[0][1]\n",
    "\n",
    "    # Early window: from first_iter to first_iter + early_window_iters\n",
    "    early_start_it = first_iter\n",
    "    early_end_it = min(first_iter + early_window_iters, last_iter)\n",
    "    km1_start = km1_at_iter(early_start_it)\n",
    "    km1_end = km1_at_iter(early_end_it)\n",
    "    early_delta = km1_start - km1_end\n",
    "    \n",
    "    if early_delta == 0:\n",
    "        # Iterate to first improvement if no improvement in early window\n",
    "        it = early_end_it + 1\n",
    "        while it <= last_iter and (it - early_start_it) <= max_iters_without_improv:\n",
    "            km1_curr = km1_at_iter(it)\n",
    "            if km1_curr < km1_end:\n",
    "                early_end_it = it\n",
    "                km1_end = km1_curr\n",
    "                early_delta = km1_start - km1_end\n",
    "                break\n",
    "            it += 1\n",
    "    \n",
    "    early_span = max(early_end_it - early_start_it, 1)\n",
    "    early_rate = early_delta / early_span\n",
    "    \n",
    "    stop_iter = last_iter\n",
    "    stop_km1 = last_km1\n",
    "    stop_time = end_time\n",
    "\n",
    "    # Slide recent window in iteration space\n",
    "    it = early_end_it\n",
    "    while it <= last_iter:\n",
    "        recent_start_it = max(first_iter, it - recent_window_iters)\n",
    "        recent_end_it = it\n",
    "        km1_recent_start = km1_at_iter(recent_start_it)\n",
    "        km1_recent_end = km1_at_iter(recent_end_it)\n",
    "        delta = km1_recent_start - km1_recent_end\n",
    "        span = max(recent_end_it - recent_start_it, 1)\n",
    "        recent_rate = delta / span\n",
    "\n",
    "        if recent_rate < alpha * early_rate:\n",
    "            stop_iter = recent_end_it\n",
    "            stop_km1 = km1_recent_end\n",
    "            stop_time = _time_at_or_before_iteration(timestamp_to_iteration, stop_iter)\n",
    "            break\n",
    "\n",
    "        it += 1\n",
    "\n",
    "    improvement_at_stop = first_km1 - stop_km1\n",
    "    frac_improvement = (\n",
    "        improvement_at_stop / total_improvement if total_improvement > 0 else 1.0\n",
    "    )\n",
    "    frac_time = (stop_time - start_time) / total_time\n",
    "\n",
    "    return stop_time, stop_iter, stop_km1, last_km1, frac_improvement, frac_time\n",
    "\n",
    "\n",
    "\n",
    "def plot_iteration_data(*datasets, title: str = \"\", labels=None, filename: str = None,\n",
    "                     show: bool = True, save: bool = False, save_dir: str = None, dpi: int = 150, iteration_to_metric: bool = False,\n",
    "                     timestamp_to_iteration: bool = False):\n",
    "    \n",
    "    assert (iteration_to_metric != timestamp_to_iteration), \"Only one of iteration_to_metric or timestamp_to_iteration can be True.\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = ['b', 'r', 'g', 'orange', 'purple', 'brown', 'pink', 'gray', 'cyan']\n",
    "    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*']\n",
    "    if labels is None:\n",
    "        labels = [f'Dataset {i+1}' for i in range(len(datasets))]  \n",
    "    for i, data in enumerate(datasets):\n",
    "        x = [entry[0] for entry in data]\n",
    "        y = [entry[1] for entry in data]\n",
    "        \n",
    "        color = colors[i % len(colors)]\n",
    "        marker = markers[i % len(markers)]\n",
    "        label = labels[i] if i < len(labels) else f'Dataset {i+1}'\n",
    "        \n",
    "        ax.plot(x, y, marker=marker, color=color, label=label, markersize=3)\n",
    "        \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    if iteration_to_metric:\n",
    "        ax.set_xlabel('Iteration', fontsize=12)\n",
    "        ax.set_ylabel('Metric Value', fontsize=12)\n",
    "    if timestamp_to_iteration:\n",
    "        ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "        ax.set_ylabel('Iteration', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if filename is None:\n",
    "        safe = title.lower().replace(' ', '_').replace('/', '_')\n",
    "        filename = f\"{safe}.png\"\n",
    "    _finalize_figure(fig, show, save, save_dir, filename, dpi)\n",
    "\n",
    "\n",
    "def plot_time_series(*datasets, title: str = \"Geometric Mean KM1 over Time\", labels=None, filename: str = None,\n",
    "                     show: bool = True, save: bool = False, save_dir: str = None, dpi: int = 300, \n",
    "                     log_x: bool = True, x_label: str = \"time $t$\", y_label: str = \"mean min $(\\lambda - 1)$\",\n",
    "                     max_time: float = None):\n",
    "    \n",
    "    # 1. Setup Figure\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    \n",
    "    # 2. Colors matching the reference PDF (Green, Blue, Brown, Teal, Orange, Purple, Red, Black)\n",
    "    academic_colors = [\"#4DAF4A\", \"#377EB8\", \"#A65628\", \"#98E0D6\", \"#FF7F00\", \"#984EA3\", \"#E41A1C\", \"#000000\"]\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = [f'Dataset {i+1}' for i in range(len(datasets))]\n",
    "        \n",
    "    # 3. Plot Data\n",
    "    for i, data in enumerate(datasets):\n",
    "        if isinstance(data, dict):\n",
    "            times = list(data.keys())\n",
    "            values = list(data.values())\n",
    "        else:\n",
    "            times = [entry[0] for entry in data]\n",
    "            values = [entry[1] for entry in data]\n",
    "        \n",
    "        # Apply max_time cutoff first\n",
    "        if max_time is not None:\n",
    "            filtered_data = [(t, v) for t, v in zip(times, values) if t <= max_time]\n",
    "            if filtered_data:\n",
    "                times, values = zip(*filtered_data)\n",
    "            else:\n",
    "                continue  # Skip this dataset if no data within max_time\n",
    "        \n",
    "        color = academic_colors[i % len(academic_colors)]\n",
    "        ax.plot(times, values, color=color, label=labels[i], linewidth=1.5)\n",
    "\n",
    "    # 4. Axis Configuration (Matplotlib side - for the PNG preview)\n",
    "    if log_x:\n",
    "        ax.set_xscale('log')\n",
    "        # Hardcode the specific ticks from your reference image\n",
    "        specific_ticks = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "        ax.xaxis.set_major_locator(FixedLocator(specific_ticks))\n",
    "        ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "    \n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.grid(True, which=\"major\", linestyle=\":\", linewidth=0.8, alpha=0.7)\n",
    "    \n",
    "    # Legend settings\n",
    "    legend = ax.legend(title=\"Algorithm\", loc='best', frameon=False)\n",
    "    plt.setp(legend.get_title(), fontsize=10, fontweight='bold')\n",
    "\n",
    "    # 5. Handle Title\n",
    "    # We show title in PNG preview, but REMOVE it for LaTeX (academic figures use \\caption below)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 6. Save Logic\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    # Save PNG (Keep title for preview)\n",
    "    if filename is None:\n",
    "        filename = \"plot_output\"\n",
    "    \n",
    "    if save:\n",
    "        png_path = os.path.join(save_dir, f\"{filename}.png\")\n",
    "        fig.savefig(png_path, dpi=dpi, bbox_inches='tight')\n",
    "        print(f\"Saved PNG to: {png_path}\")\n",
    "\n",
    "    # 7. Save TikZ (The Critical Part for LaTeX)\n",
    "    if save:\n",
    "        tex_path = os.path.join(save_dir, f\"{filename}.tex\")\n",
    "        \n",
    "        # Remove title specifically for the TeX file so it doesn't appear at the top\n",
    "        current_title = ax.get_title()\n",
    "        ax.set_title(\"\") \n",
    "        \n",
    "        # PGFPlots commands to force the style\n",
    "        extra_axis_opts = [\n",
    "            'height=7cm',\n",
    "            'width=9cm',\n",
    "            'grid=both',\n",
    "            'grid style={dotted, gray!50}',  # Matches the faint dotted grid\n",
    "            'major grid style={dotted, gray!50}',\n",
    "            'xlabel near ticks',\n",
    "            'ylabel near ticks',\n",
    "            'tick align=outside',\n",
    "            'legend cell align={left}',\n",
    "            'legend style={draw=none, fill=none, font=\\\\small}', # No box around legend\n",
    "            'x label style={font=\\\\large}',\n",
    "            'y label style={font=\\\\large}',\n",
    "        ]\n",
    "        \n",
    "        # Explicitly force Log Mode and Ticks for PGFPlots\n",
    "        if log_x:\n",
    "            extra_axis_opts.extend([\n",
    "                'xmode=log',\n",
    "                'log ticks with fixed point', # Shows \"10\" instead of \"10^1\"\n",
    "                'xtick={1, 2, 5, 10, 20, 50, 100, 200, 500}', # Exact ticks from reference\n",
    "            ])\n",
    "\n",
    "        tikzplotlib.save(\n",
    "            tex_path,\n",
    "            axis_height='7cm',\n",
    "            axis_width='9cm',\n",
    "            extra_axis_parameters=extra_axis_opts\n",
    "        )\n",
    "        \n",
    "        # Restore title in case plot object is reused (optional)\n",
    "        ax.set_title(current_title)\n",
    "        print(f\"Saved TikZ to: {tex_path}\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_single_matrix(matrix, title: str = \"Difference Matrix\", filename: str = None,\n",
    "                       show: bool = True, save: bool = False, save_dir: str = None, dpi: int = 150):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    if not isinstance(matrix, np.ndarray):\n",
    "        matrix = np.array(matrix)\n",
    "    \n",
    "    im = ax.imshow(matrix, cmap='viridis', aspect='auto', interpolation='nearest')\n",
    "    fig.colorbar(im, ax=ax, label='Difference Value')\n",
    "\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if filename is None:\n",
    "        safe = title.lower().replace(' ', '_').replace('/', '_')\n",
    "        filename = f\"{safe}.png\"\n",
    "    _finalize_figure(fig, show, save, save_dir, filename, dpi)\n",
    "    \n",
    "def plot_frac_time_vs_improvement(points, title: str, filename: str = None,\n",
    "                                  show: bool = True, save: bool = False,\n",
    "                                  save_dir: str = None, dpi: int = 150):\n",
    "    if not points:\n",
    "        return\n",
    "\n",
    "    xs = [p[0] for p in points]\n",
    "    ys = [p[1] for p in points]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sc = ax.scatter(xs, ys, c=ys, cmap=\"viridis\", s=20, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel(\"Fraction of time used\", fontsize=12)\n",
    "    ax.set_ylabel(\"Fraction of improvement achieved\", fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Always use fixed 0..1 ranges\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    cbar = fig.colorbar(sc, ax=ax)\n",
    "    cbar.set_label(\"Fraction of improvement\", fontsize=12)\n",
    "\n",
    "    # Optional guide lines\n",
    "    ax.axvline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.4)\n",
    "    ax.axhline(0.8, color=\"gray\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if filename is None:\n",
    "        safe = title.lower().replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        filename = f\"{safe}.png\"\n",
    "    _finalize_figure(fig, show, save, save_dir, filename, dpi)\n",
    "\n",
    "# --- Bulk helpers ---\n",
    "\n",
    "def save_multiple_time_series(series_groups, titles, labels_groups=None, save_dir=None, base_filename=None, dpi=150, show=True):\n",
    "    for i, group in enumerate(series_groups):\n",
    "        title = titles[i] if i < len(titles) else f\"series_{i+1}\"\n",
    "        labels = None\n",
    "        if labels_groups and i < len(labels_groups):\n",
    "            labels = labels_groups[i]\n",
    "        # Normalize group to tuple of datasets\n",
    "        if isinstance(group, (list, tuple)) and group and isinstance(group[0], (list, tuple)):\n",
    "            datasets = group\n",
    "        else:\n",
    "            datasets = (group,)\n",
    "        filename = None\n",
    "        if base_filename:\n",
    "            filename = f\"{base_filename}_{i+1}.png\"\n",
    "        plot_time_series(*datasets, title=title, labels=labels, filename=filename, show=show, save=True, save_dir=save_dir, dpi=dpi)\n",
    "\n",
    "\n",
    "def save_multiple_matrices(matrices, titles, save_dir=None, base_filename=None, dpi=150, show=True):\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        title = titles[i] if i < len(titles) else f\"matrix_{i+1}\"\n",
    "        filename = None\n",
    "        if base_filename:\n",
    "            filename = f\"{base_filename}_{i+1}.png\"\n",
    "        plot_single_matrix(matrix, title=title, filename=filename, show=show, save=True, save_dir=save_dir, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d085d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILES_DIR = f\"{HOME_DIR}/Documents/experiment_results/2025-11-13_long_diff_combined_results/2025-11-13_long_diff_test/mt_kahypar_evo_results\"\n",
    "DIFF_FILES_DIR = f\"{HOME_DIR}/Documents/experiment_results/2025-11-13_long_diff_combined_results/2025-11-13_long_diff_test/evo_diff\"\n",
    "HISTORY_FILES_DIR = f\"{HOME_DIR}/Documents/experiment_results/2025-11-13_long_diff_combined_results/2025-11-13_long_diff_test/evo_history\"\n",
    "\n",
    "SHORT_TIMELIMIT = None\n",
    "LONG_TIMELIMIT = 21600\n",
    "\n",
    "\n",
    "# Aggregate runs from result files\n",
    "runs = aggregate_runs(RESULT_FILES_DIR)\n",
    "k8_runs, k32_runs = split_runs_k_value(runs, '8', '32')\n",
    "\n",
    "diff_array_k2 = []\n",
    "for instance_name, results in k32_runs.items():\n",
    "    short_km1 = results[\"short\"]\n",
    "    long_km1 = results[\"long\"]\n",
    "    if short_km1 is not None and long_km1 is not None:\n",
    "        diff = long_km1 - short_km1\n",
    "        diff_array_k2.append(diff)\n",
    "diff_array_k8 = []\n",
    "for instance_name, results in k8_runs.items():\n",
    "    short_km1 = results[\"short\"]\n",
    "    long_km1 = results[\"long\"]\n",
    "    if short_km1 is not None and long_km1 is not None:\n",
    "        diff = long_km1 - short_km1\n",
    "        diff_array_k8.append(diff)\n",
    "\n",
    "# How often is long run better than short runs (percentage)\n",
    "if diff_array_k2:\n",
    "    better_count = sum(1 for d in diff_array_k2 if d < 0)\n",
    "    same_count = sum(1 for d in diff_array_k2 if d == 0)\n",
    "    percentage_better = (better_count / len(diff_array_k2)) * 100\n",
    "    percentage_same = (same_count / len(diff_array_k2)) * 100\n",
    "    print(f\"Percentage of instances where long run is better than short runs (k=32): {percentage_better:.2f}%\")\n",
    "    print(f\"Percentage of instances where long run is the same as short runs (k=32): {percentage_same:.2f}%\")\n",
    "    # better_count_k8 = sum(1 for d in diff_array_k8 if d < 0)\n",
    "    # percentage_better_k8 = (better_count_k8 / len(diff_array_k8)) * 100\n",
    "    # print(f\"Percentage of instances where long run is better than short runs (k=8): {percentage_better_k8:.2f}%\")\n",
    "else:\n",
    "    print(\"No valid differences to analyze.\")\n",
    "\n",
    "\n",
    "# Get Geomean for short and long runs\n",
    "short_km1_values_k32 = [km1[\"short\"] for km1 in k32_runs.values() if km1[\"short\"] is not None]\n",
    "long_km1_values_k32 = [km1[\"long\"] for km1 in k32_runs.values() if km1[\"long\"] is not None]\n",
    "if short_km1_values_k32 and long_km1_values_k32:\n",
    "    geomean_short = geometric_mean(short_km1_values_k32)\n",
    "    geomean_long = geometric_mean(long_km1_values_k32)\n",
    "    print(f\"Geometric Mean KM1 (k=32) - Short Runs: {geomean_short}, Long Runs: {geomean_long}\")\n",
    "    \n",
    "short_km1_values_k8 = [km1[\"short\"] for km1 in k8_runs.values() if km1[\"short\"] is not None]\n",
    "long_km1_values_k8 = [km1[\"long\"] for km1 in k8_runs.values() if km1[\"long\"] is not None]\n",
    "if short_km1_values_k8 and long_km1_values_k8:\n",
    "    geomean_short_k8 = geometric_mean(short_km1_values_k8)\n",
    "    geomean_long_k8 = geometric_mean(long_km1_values_k8)\n",
    "    print(f\"Geometric Mean KM1 (k=8) - Short Runs: {geomean_short_k8}, Long Runs: {geomean_long_k8}\")\n",
    "\n",
    "# Analyze largest relative differences (in favor of long runs)\n",
    "relative_diffs = []\n",
    "for run, km1 in k32_runs.items():\n",
    "    short_km1 = km1[\"short\"]\n",
    "    long_km1 = km1[\"long\"]\n",
    "    if short_km1 is not None and long_km1 is not None:\n",
    "        diff = long_km1 - short_km1\n",
    "        relative_diff = diff / short_km1\n",
    "        relative_diffs.append((run, relative_diff))\n",
    "    elif short_km1 is None and long_km1 is not None:\n",
    "        relative_diffs.append((run, float('inf')))\n",
    "relative_diffs.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"relative differences: \", relative_diffs[:10])\n",
    "\n",
    "\n",
    "# Diff Matrices Analysis\n",
    "diff_runs = aggregate_diff_runs(DIFF_FILES_DIR)\n",
    "\n",
    "# Analyze diff matrices for worst long runs\n",
    "show_full_history = False\n",
    "\n",
    "history_runs = aggregate_history_runs(HISTORY_FILES_DIR, full_history=show_full_history)\n",
    "for run, _ in relative_diffs[:10]:\n",
    "    short_instance = f\"{run}.{SHORT_TIMELIMIT}\"\n",
    "    long_instance = f\"{run}.{LONG_TIMELIMIT}\"   \n",
    "    if long_instance in diff_runs:\n",
    "        \n",
    "        #short_matrices = diff_runs[short_instance]\n",
    "        long_matrices = diff_runs[long_instance]\n",
    "\n",
    "        if len(long_matrices[0]['matrices']) == 0:\n",
    "            continue\n",
    "        #if len(short_matrices[0]['matrices']) == 0:\n",
    "        #    continue\n",
    "\n",
    "        #best_short_matrices = get_diff_matrices_for_best_run(history_runs, short_matrices, short_instance)\n",
    "        #last_short_matrix = best_short_matrices[-1] if best_short_matrices else None\n",
    "        last_long_matrix = long_matrices[0]['matrices'][-1]\n",
    "        \n",
    "        long_diff_run = long_matrices[0]['matrices']\n",
    "        #short_diff_run = best_short_matrices\n",
    "        #short_history_run = history_runs[short_instance]['history']\n",
    "        long_history_run = history_runs[long_instance]['history']\n",
    "        \n",
    "        #combined_short = combine_history_and_diff(short_history_run, short_diff_run)\n",
    "        combined_long = combine_history_and_diff(long_history_run, long_diff_run, time_limit=LONG_TIMELIMIT)\n",
    "        #plot_combined_data(combined_short, title=f\"Short Run Combined Data for {short_instance}\")\n",
    "        plot_combined_data(combined_long, title=f\"Long Run Combined Data for {long_instance}\")\n",
    "\n",
    "        if last_long_matrix:\n",
    "            #plot_single_matrix(last_short_matrix, title=f\"Short Run Diff Matrix for {short_instance}\")\n",
    "            plot_single_matrix(last_long_matrix, title=f\"Long Run Diff Matrix for {long_instance}\")\n",
    "        \n",
    "# Geometric Mean over all instances\n",
    "short_history_runs, long_history_runs = split_long_short_history_runs(history_runs)\n",
    "short_k8_runs, short_k32_runs = split_k_value_history_runs(short_history_runs, '8', '32')\n",
    "long_k8_runs, long_k32_runs = split_k_value_history_runs(long_history_runs, '8', '32')\n",
    "# geomean_short = create_geomean_over_all_instances(short_k32_runs)\n",
    "geomean_long = create_geomean_over_all_instances(long_k32_runs)\n",
    "# plot_time_series(geomean_short, geomean_long, \n",
    "#                  title=\"Geometric Mean KM1 over Time (Short vs Long Runs)\",\n",
    "#                  labels=['Short Runs', 'Long Runs'])\n",
    "\n",
    "# Show full history\n",
    "show_full_history = True\n",
    "history_runs = aggregate_history_runs(HISTORY_FILES_DIR, full_history=show_full_history)\n",
    "short_history_runs_full, _ = split_long_short_history_runs(history_runs)\n",
    "_, short_k32_runs_full = split_k_value_history_runs(short_history_runs_full, '8', '32')\n",
    "all_seeds_short_histories_k32 = split_seed_history_runs(short_k32_runs_full)\n",
    "\n",
    "fixed_short_history_runs = make_history_runs_sequential(*all_seeds_short_histories_k32, time_limit=SHORT_TIMELIMIT)\n",
    "#fixed_geomean_short = create_geomean_over_all_instances(fixed_short_history_runs)\n",
    "\n",
    "geomean_for_all_seeds = []\n",
    "for seed_runs in all_seeds_short_histories_k32:\n",
    "    geomean = create_geomean_over_all_instances(seed_runs)\n",
    "    geomean_for_all_seeds.append(geomean)\n",
    "plot_time_series(*geomean_for_all_seeds, geomean_long,\n",
    "                 title=\"Geometric Mean KM1 over Time (k=32)\",\n",
    "                 labels=[f'Seed {i+1}' for i in range(len(geomean_for_all_seeds))] + ['Long Runs'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS_NAME = \"2025-11-28_combined_results\"\n",
    "ALL_CONFIGS_DIR = os.path.expanduser(f\"~/Documents/experiment_results/{CONFIGS_NAME}\")\n",
    "RESULTS = \"mt_kahypar_evo_results\"\n",
    "DIFF = \"evo_diff\"\n",
    "HISTORY = \"evo_history\"\n",
    "DEFAULT_POP_SIZE = 10\n",
    "show_full_history = True\n",
    "\n",
    "\n",
    "INSTANCES_TO_EXCLUDE = [\"Pd_rhs.mtx.hgr\", \"wb-edu.mtx.hgr\"]\n",
    "\n",
    "k_pop_to_kway_geomeans, save_path = eval_k_kway(configs_name=CONFIGS_NAME,\n",
    "                                         all_configs_root=os.path.expanduser(\"~/Documents/experiment_results\"),\n",
    "                                         results_dirname=RESULTS,\n",
    "                                         diff_dirname=DIFF,\n",
    "                                         history_dirname=HISTORY,\n",
    "                                         instances_to_exclude=INSTANCES_TO_EXCLUDE,\n",
    "                                         show_full_history=show_full_history,\n",
    "                                         default_pop_size=\"dynamic\")\n",
    "\n",
    "for (k, pop_size), kway_dict in k_pop_to_kway_geomeans.items():\n",
    "    datasets = []\n",
    "    labels = []\n",
    "    \n",
    "    # Sort kway keys for consistent legend order if desired\n",
    "    sorted_kways = sorted(kway_dict.keys(), key=lambda x: str(x)) # simple sort, might need int conversion if kway is numeric string\n",
    "\n",
    "    for kway in sorted_kways:\n",
    "        geomean = kway_dict[kway]\n",
    "        datasets.append(geomean)\n",
    "        labels.append(f'kway={kway}')\n",
    "\n",
    "    if datasets:\n",
    "        plot_time_series(*datasets,\n",
    "                         title=f\"Geometric Mean KM1 over Time (k={k}, pop={pop_size})\",\n",
    "                         labels=labels,\n",
    "                         save=True,\n",
    "                         save_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b52ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS_NAME = \"2025-11-29_parallel_combined_results\"\n",
    "ALL_CONFIGS_DIR = os.path.expanduser(f\"~/Documents/BA_results/{CONFIGS_NAME}\")\n",
    "RESULTS = \"mt_kahypar_evo_results\"\n",
    "DIFF = \"evo_diff\"\n",
    "HISTORY = \"evo_history\"\n",
    "ITERATION_LOG = \"evo_iteration_log\"\n",
    "DEFAULT_POP_SIZE = 10\n",
    "INSTANCES_TO_EXCLUDE = [\"Pd_rhs.mtx.hgr\"]\n",
    "show_full_history = True\n",
    "\n",
    "\n",
    "threads_per_worker_to_geomeans, save_path, max_threads = eval_evothreads(\n",
    "    configs_name=CONFIGS_NAME,\n",
    "    all_configs_root=os.path.expanduser(\"~/Documents/experiment_results\"),\n",
    "    results_dirname=RESULTS,\n",
    "    diff_dirname=DIFF,\n",
    "    history_dirname=HISTORY,\n",
    "    instances_to_exclude=INSTANCES_TO_EXCLUDE,\n",
    "    show_full_history=show_full_history,\n",
    ")\n",
    "\n",
    "all_aggregated_iteration_runs = {}\n",
    "all_aggregated_history_runs = {}\n",
    "for config_dir in glob.glob(ALL_CONFIGS_DIR + \"/*/\"):\n",
    "    iteration_log_path = os.path.join(config_dir, ITERATION_LOG)\n",
    "    history_path = os.path.join(config_dir, HISTORY)\n",
    "    config_name = os.path.basename(os.path.normpath(config_dir))\n",
    "    config_name = remove_date_prefix(config_name)\n",
    "    all_aggregated_iteration_runs[config_name] = aggregate_iterations_runs(iteration_log_path, show_full_history=show_full_history, instances_to_exclude=INSTANCES_TO_EXCLUDE)\n",
    "    all_aggregated_history_runs[config_name] = aggregate_history_runs(history_path, full_history=show_full_history, instances_to_exclude=INSTANCES_TO_EXCLUDE)\n",
    "\n",
    "# Create geomeans for iterations\n",
    "all_geomeans_iterations = {}\n",
    "baseline_config = None\n",
    "baseline_iterations = None\n",
    "current_max = 0\n",
    "for config_name, iteration_runs in all_aggregated_iteration_runs.items():\n",
    "    max_iterations = max_iterations_per_instance(\n",
    "        iteration_runs, multiple_seeds=show_full_history\n",
    "    )\n",
    "    if not max_iterations:\n",
    "        continue\n",
    "    avg_iters = mean(max_iterations.values())\n",
    "    if avg_iters > current_max:\n",
    "        current_max = avg_iters\n",
    "        baseline_config = config_name\n",
    "        baseline_iterations = max_iterations\n",
    "\n",
    "# Calculate multipliers for each config based on baseline       \n",
    "for config_name, iteration_runs in all_aggregated_iteration_runs.items():\n",
    "    max_iterations = max_iterations_per_instance(iteration_runs, multiple_seeds=show_full_history)\n",
    "    multipliers = get_iteration_multipliers(baseline_max_iterations=baseline_iterations, other_max_iterations=max_iterations)\n",
    "\n",
    "    adjusted_history_runs = create_multiplier_based_history_runs(all_aggregated_history_runs[config_name], multipliers)\n",
    "    all_geomeans_iterations[config_name] = create_geomean_over_all_instances(adjusted_history_runs, multiple_seeds=show_full_history)\n",
    "\n",
    "\n",
    "# Plot all in one plot\n",
    "labels_for_all = []\n",
    "for config_name, geomean in threads_per_worker_to_geomeans.items():\n",
    "    labels_for_all.append(config_name)\n",
    "plot_time_series(*threads_per_worker_to_geomeans.values(), \n",
    "                     title=\"Geometric Mean KM1 over actual Time for Modified Combine Strategies\",\n",
    "                     labels=labels_for_all,\n",
    "                     save=True,\n",
    "                     save_dir=save_path)\n",
    "\n",
    "labels_for_all = []\n",
    "for config_name, geomean in all_geomeans_iterations.items():\n",
    "    labels_for_all.append(config_name)\n",
    "plot_time_series(*all_geomeans_iterations.values(), \n",
    "                     title=\"Geometric Mean KM1 over iteration normalized Time for Modified Combine Strategies\",\n",
    "                     labels=labels_for_all,\n",
    "                     save=True,\n",
    "                     save_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88534bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS_NAME = \"2026-1-5_meta\"\n",
    "ALL_CONFIGS_DIR = os.path.expanduser(f\"~/Documents/BA_results/{CONFIGS_NAME}\")\n",
    "RESULTS = \"mt_kahypar_evo_results\"\n",
    "DIFF = \"evo_diff\"\n",
    "HISTORY = \"evo_history\"\n",
    "ITERATION_LOG = \"evo_iteration_log\"\n",
    "DEFAULT_POP_SIZE = 10\n",
    "INSTANCES_TO_EXCLUDE = [\"Pd_rhs.mtx.hgr\"]\n",
    "MAX_TIME = 7200\n",
    "show_full_history = True\n",
    "\n",
    "time_normalized = False\n",
    "\n",
    "modifiedCombines_to_geomeans, save_path = eval_generic_by_name(\n",
    "    configs_name=CONFIGS_NAME,\n",
    "    all_configs_root=os.path.expanduser(\"~/Documents/BA_results\"),\n",
    "    results_dirname=RESULTS,\n",
    "    diff_dirname=DIFF,\n",
    "    history_dirname=HISTORY,\n",
    "    instances_to_exclude=INSTANCES_TO_EXCLUDE,\n",
    "    show_full_history=show_full_history)\n",
    "\n",
    "# Go through all config directories\n",
    "all_aggregated_iteration_runs = {}\n",
    "all_aggregated_history_runs = {}\n",
    "for config_dir in glob.glob(ALL_CONFIGS_DIR + \"/*/\"):\n",
    "    iteratiion_log_path = os.path.join(config_dir, ITERATION_LOG)\n",
    "    history_path = os.path.join(config_dir, HISTORY)\n",
    "    config_name = os.path.basename(os.path.normpath(config_dir))\n",
    "    config_name = remove_date_prefix(config_name)\n",
    "    all_aggregated_iteration_runs[config_name] = aggregate_iterations_runs(iteratiion_log_path, show_full_history=show_full_history, instances_to_exclude=INSTANCES_TO_EXCLUDE)\n",
    "    all_aggregated_history_runs[config_name] = aggregate_history_runs(history_path, full_history=show_full_history, instances_to_exclude=INSTANCES_TO_EXCLUDE)\n",
    "\n",
    "# Create geomeans for iterations\n",
    "all_geomeans_iterations = {}\n",
    "current_max = 0\n",
    "for config_name, iteration_runs in all_aggregated_iteration_runs.items():\n",
    "    max_iterations = max_iterations_per_instance(iteration_runs, multiple_seeds=show_full_history)\n",
    "    if \"default\" in config_name:\n",
    "        baseline_iterations = max_iterations\n",
    "        break\n",
    "\n",
    "# Calculate multipliers for each config based on baseline     \n",
    "if time_normalized:  \n",
    "    for config_name, iteration_runs in all_aggregated_iteration_runs.items():\n",
    "        max_iterations = max_iterations_per_instance(iteration_runs, multiple_seeds=show_full_history)\n",
    "        multipliers = get_iteration_multipliers(baseline_max_iterations=baseline_iterations, other_max_iterations=max_iterations)\n",
    "\n",
    "        adjusted_history_runs = create_multiplier_based_history_runs(all_aggregated_history_runs[config_name], multipliers)\n",
    "        all_geomeans_iterations[config_name] = create_geomean_over_all_instances(adjusted_history_runs, multiple_seeds=show_full_history)\n",
    "\n",
    "\n",
    "# Plot all in one plot\n",
    "labels_for_all = []\n",
    "for config_name, geomean in modifiedCombines_to_geomeans.items():\n",
    "    labels_for_all.append(config_name)\n",
    "plot_time_series(*modifiedCombines_to_geomeans.values(), \n",
    "                     title=\"Geometric Mean KM1 over all instances\",\n",
    "                     labels=labels_for_all,\n",
    "                     save=True,\n",
    "                     save_dir=save_path,\n",
    "                     log_x=False,\n",
    "                     max_time=MAX_TIME\n",
    "                 )\n",
    "\n",
    "# labels_for_all = []\n",
    "# for config_name, geomean in all_geomeans_iterations.items():\n",
    "#     labels_for_all.append(config_name)\n",
    "# plot_time_series(*all_geomeans_iterations.values(), \n",
    "#                      title=\"Geometric Mean KM1 over iteration normalized Time for Modified Combine Strategies\",\n",
    "#                      labels=labels_for_all,\n",
    "#                      save=True,\n",
    "#                      save_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping Criteria Evaluation ####\n",
    "CONFIGS_NAME = \"2025-11-25_combined_results\"\n",
    "ALL_CONFIGS_DIR = os.path.expanduser(f\"~/Documents/experiment_results/{CONFIGS_NAME}\")\n",
    "RESULTS = \"mt_kahypar_evo_results\"\n",
    "DIFF = \"evo_diff\"\n",
    "HISTORY = \"evo_history\"\n",
    "ITERATION_LOG = \"evo_iteration_log\"\n",
    "DEFAULT_POP_SIZE = 10\n",
    "INSTANCES_TO_EXCLUDE = [\"Pd_rhs.mtx.hgr\", \"wb-edu.mtx.hgr\"]\n",
    "show_full_history = True\n",
    "REFERENCE_CONFIG = \"2025-11-25_defaultCombine\"\n",
    "\n",
    "from statistics import mean\n",
    "import json\n",
    "\n",
    "\n",
    "history_runs = aggregate_history_runs(os.path.join(ALL_CONFIGS_DIR, REFERENCE_CONFIG, HISTORY), full_history=show_full_history, instances_to_exclude=INSTANCES_TO_EXCLUDE)\n",
    "iteration_runs = aggregate_iterations_runs(os.path.join(ALL_CONFIGS_DIR, REFERENCE_CONFIG, ITERATION_LOG), show_full_history=show_full_history, instances_to_exclude=INSTANCES_TO_EXCLUDE)\n",
    "\n",
    "\n",
    "\n",
    "# 1) Simulate \"last K iterations without improvement\" stopping criteria\n",
    "max_k_values = [10, 20, 50, 65, 100]\n",
    "stopping_criteria_stats = {k: [] for k in max_k_values}\n",
    "stopping_criteria_summary = {} \n",
    "\n",
    "for instance_name, iteration_data in iteration_runs.items():\n",
    "    timestamp_to_iteration, iteration_to_km1 = iteration_data\n",
    "    if len(iteration_to_km1) < 2:\n",
    "        continue\n",
    "\n",
    "    for k in max_k_values:\n",
    "        res = simulate_stop_after_k_no_improvement_iterations(\n",
    "            timestamp_to_iteration, iteration_to_km1, k\n",
    "        )\n",
    "        stop_time, stop_iter, stop_km1, final_km1, frac_improvement, frac_time = res\n",
    "        if stop_time is None:\n",
    "            continue\n",
    "        stopping_criteria_stats[k].append({\n",
    "            \"instance\": instance_name,\n",
    "            \"stop_time\": stop_time,\n",
    "            \"stop_iter\": stop_iter,\n",
    "            \"stop_km1\": stop_km1,\n",
    "            \"final_km1\": final_km1,\n",
    "            \"frac_improvement\": frac_improvement,\n",
    "            \"frac_time\": frac_time,\n",
    "        })\n",
    "for k in max_k_values:\n",
    "    stats = stopping_criteria_stats[k]\n",
    "    if not stats:\n",
    "        continue\n",
    "\n",
    "    avg_frac_impr = mean(s[\"frac_improvement\"] for s in stats)\n",
    "    min_frac_impr = min(s[\"frac_improvement\"] for s in stats)\n",
    "    avg_frac_time = mean(s[\"frac_time\"] for s in stats)\n",
    "\n",
    "    print(f\"K={k}: \"\n",
    "          f\"avg frac_improvement={avg_frac_impr:.3f}, \"\n",
    "          f\"min frac_improvement={min_frac_impr:.3f}, \"\n",
    "          f\"avg frac_time={avg_frac_time:.3f}, \"\n",
    "          f\"num_instances={len(stats)}\")\n",
    "    \n",
    "    stopping_criteria_summary[k] = {\n",
    "        \"avg_frac_improvement\": avg_frac_impr,\n",
    "        \"min_frac_improvement\": min_frac_impr,\n",
    "        \"avg_frac_time\": avg_frac_time,\n",
    "        \"num_instances\": len(stats),\n",
    "    }\n",
    "    \n",
    "    # scatter plot per instance\n",
    "    points = [(s[\"frac_time\"], s[\"frac_improvement\"]) for s in stats]\n",
    "    plot_frac_time_vs_improvement(\n",
    "        points,\n",
    "        title=f\"K={k}: frac_time vs frac_improvement\",\n",
    "        save=True,\n",
    "        save_dir=FIG_SAVE_DIR,\n",
    "    )\n",
    "\n",
    "\n",
    "# 2) Simulate \"sliding window improvement rate\" stopping criteria\n",
    "# (early window, recent window, alpha, max iters without improv)\n",
    "rate_params = [\n",
    "    (5, 5, 0.1, 100),\n",
    "    (5, 5, 0.05, 100),\n",
    "    (5, 5, 0.1, 200),\n",
    "    (5, 5, 0.05, 200),\n",
    "    (5, 5, 0.1, 300),\n",
    "]\n",
    "rate_summary = {}\n",
    "for params in rate_params:\n",
    "    ew, rw, alpha, max_iters_without_improv = params\n",
    "    stats_for_param = []\n",
    "    for instance_name, iteration_data in iteration_runs.items():\n",
    "        timestamp_to_iteration, iteration_to_km1 = iteration_data\n",
    "        if len(iteration_to_km1) < 2:\n",
    "            continue\n",
    "\n",
    "        res = simulate_stop_by_improvement_rate(\n",
    "            timestamp_to_iteration, iteration_to_km1,\n",
    "            early_window_improvs=ew,\n",
    "            recent_window_improvs=rw,\n",
    "            alpha=alpha,\n",
    "            max_iters_without_improv=max_iters_without_improv,\n",
    "        )\n",
    "        stop_time, stop_iter, stop_km1, final_km1, frac_improvement, frac_time = res\n",
    "        if stop_time is None:\n",
    "            continue\n",
    "        stats_for_param.append({\n",
    "            \"instance\": instance_name,\n",
    "            \"frac_improvement\": frac_improvement,\n",
    "            \"frac_time\": frac_time,\n",
    "        })\n",
    "\n",
    "    if stats_for_param:\n",
    "        avg_frac_improvement = mean(s[\"frac_improvement\"] for s in stats_for_param)\n",
    "        min_frac_impr = min(s[\"frac_improvement\"] for s in stats_for_param)\n",
    "        avg_frac_time = mean(s[\"frac_time\"] for s in stats_for_param)\n",
    "\n",
    "        print(\n",
    "            f\"Sliding window improvements (early={ew}, recent={rw}, alpha={alpha}, \"\n",
    "            f\"max_iters_without_improv={max_iters_without_improv}): \"\n",
    "            f\"avg frac_improvement={avg_frac_improvement:.3f}, \"\n",
    "            f\"min frac_improvement={min_frac_impr:.3f}, \"\n",
    "            f\"avg frac_time={avg_frac_time:.3f}, \"\n",
    "            f\"num_instances={len(stats_for_param)}\"\n",
    "        )\n",
    "        rate_summary[str(params)] = {\n",
    "            \"early_window_improvs\": ew,\n",
    "            \"recent_window_improvs\": rw,\n",
    "            \"alpha\": alpha,\n",
    "            \"max_iters_without_improv\": max_iters_without_improv,\n",
    "            \"avg_frac_improvement\": avg_frac_improvement,\n",
    "            \"min_frac_improvement\": min_frac_impr,\n",
    "            \"avg_frac_time\": avg_frac_time,\n",
    "            \"num_instances\": len(stats_for_param),\n",
    "        }\n",
    "        points = [(s[\"frac_time\"], s[\"frac_improvement\"]) for s in stats_for_param]\n",
    "        plot_frac_time_vs_improvement(\n",
    "            points,\n",
    "            title=f\"Sliding window (early={ew}, recent={rw}, alpha={alpha}, max_idle={max_iters_without_improv})\",\n",
    "            save=True,\n",
    "            save_dir=FIG_SAVE_DIR,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# 3) Simulate \"sliding window iteration rate\" stopping criteria\n",
    "# (early window iters, recent window iters, alpha)\n",
    "iter_rate_params = [\n",
    "    (20, 200, 0.1),\n",
    "    (50, 200, 0.1),\n",
    "    (15, 100, 0.05),]\n",
    "\n",
    "rate_stats = {p: [] for p in iter_rate_params}\n",
    "iter_rate_summary = {}\n",
    "for instance_name, iteration_data in iteration_runs.items():\n",
    "    timestamp_to_iteration, iteration_to_km1 = iteration_data\n",
    "    if len(iteration_to_km1) < 2:\n",
    "        continue\n",
    "\n",
    "    for params in iter_rate_params:\n",
    "        ew, rw, alpha = params\n",
    "        res = simulate_stop_by_improvement_rate_iter_window(\n",
    "            timestamp_to_iteration, iteration_to_km1,\n",
    "            early_window_iters=ew,\n",
    "            recent_window_iters=rw,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        stop_time, stop_iter, stop_km1, final_km1, frac_improvement, frac_time = res\n",
    "        if stop_time is None:\n",
    "            continue\n",
    "        rate_stats[params].append({\n",
    "            \"instance\": instance_name,\n",
    "            \"stop_time\": stop_time,\n",
    "            \"stop_iter\": stop_iter,\n",
    "            \"stop_km1\": stop_km1,\n",
    "            \"final_km1\": final_km1,\n",
    "            \"frac_improvement\": frac_improvement,\n",
    "            \"frac_time\": frac_time,\n",
    "        })\n",
    "for params, stats in rate_stats.items():\n",
    "    if not stats:\n",
    "        continue\n",
    "    ew, rw, alpha = params\n",
    "    avg_frac_impr = mean(s[\"frac_improvement\"] for s in stats)\n",
    "    min_frac_impr = min(s[\"frac_improvement\"] for s in stats)\n",
    "    avg_frac_time = mean(s[\"frac_time\"] for s in stats)\n",
    "\n",
    "    print(f\"Sliding window iterations (early={ew}, recent={rw}, alpha={alpha}): \"\n",
    "          f\"avg frac_improvement={avg_frac_impr:.3f}, \"\n",
    "          f\"min frac_improvement={min_frac_impr:.3f}, \"\n",
    "          f\"avg frac_time={avg_frac_time:.3f}, \"\n",
    "          f\"num_instances={len(stats)}\")\n",
    "\n",
    "    iter_rate_summary[str(params)] = {\n",
    "        \"early_window_iters\": ew,\n",
    "        \"recent_window_iters\": rw,\n",
    "        \"alpha\": alpha,\n",
    "        \"avg_frac_improvement\": avg_frac_impr,\n",
    "        \"min_frac_improvement\": min_frac_impr,\n",
    "        \"avg_frac_time\": avg_frac_time,\n",
    "        \"num_instances\": len(stats),\n",
    "    }\n",
    "\n",
    "    points = [(s[\"frac_time\"], s[\"frac_improvement\"]) for s in stats]\n",
    "    plot_frac_time_vs_improvement(\n",
    "        points,\n",
    "        title=f\"Iter-window (early={ew}, recent={rw}, alpha={alpha})\",\n",
    "        save=True,\n",
    "        save_dir=FIG_SAVE_DIR,\n",
    "    )\n",
    "\n",
    "\n",
    "# 4) Simulate \"global tangent\" stopping criteria\n",
    "global_tangent_params = [\n",
    "    (0.35, 50, 300),  # (alpha, min_iters_for_rate, max_iters_without_improv)\n",
    "]\n",
    "\n",
    "global_stats = {p: [] for p in global_tangent_params}\n",
    "global_summary = {}\n",
    "for instance_name, iteration_data in iteration_runs.items():\n",
    "    timestamp_to_iteration, iteration_to_km1 = iteration_data\n",
    "    if len(iteration_to_km1) < 2:\n",
    "        continue\n",
    "\n",
    "    for params in global_tangent_params:\n",
    "        alpha, min_iters_for_rate, max_idle = params\n",
    "        res = simulate_stop_by_global_tangent(\n",
    "            timestamp_to_iteration,\n",
    "            iteration_to_km1,\n",
    "            alpha=alpha,\n",
    "            min_iters=min_iters_for_rate,\n",
    "            max_iters_without_improv=max_idle,\n",
    "        )\n",
    "        stop_time, stop_iter, stop_km1, final_km1, frac_improvement, frac_time = res\n",
    "        if stop_time is None:\n",
    "            continue\n",
    "        global_stats[params].append(\n",
    "            {\n",
    "                \"instance\": instance_name,\n",
    "                \"stop_time\": stop_time,\n",
    "                \"stop_iter\": stop_iter,\n",
    "                \"stop_km1\": stop_km1,\n",
    "                \"final_km1\": final_km1,\n",
    "                \"frac_improvement\": frac_improvement,\n",
    "                \"frac_time\": frac_time,\n",
    "            }\n",
    "        )\n",
    "\n",
    "for params, stats in global_stats.items():\n",
    "    if not stats:\n",
    "        continue\n",
    "    alpha, min_iters_for_rate, max_idle = params\n",
    "    avg_frac_impr = mean(s[\"frac_improvement\"] for s in stats)\n",
    "    min_frac_impr = min(s[\"frac_improvement\"] for s in stats)\n",
    "    avg_frac_time = mean(s[\"frac_time\"] for s in stats)\n",
    "\n",
    "    print(\n",
    "        f\"Global tangent (alpha={alpha}, min_iters_for_rate={min_iters_for_rate}, \"\n",
    "        f\"max_iters_without_improv={max_idle}): \"\n",
    "        f\"avg frac_improvement={avg_frac_impr:.3f}, \"\n",
    "        f\"min frac_improvement={min_frac_impr:.3f}, \"\n",
    "        f\"avg frac_time={avg_frac_time:.3f}, \"\n",
    "        f\"num_instances={len(stats)}\"\n",
    "    )\n",
    "    global_summary[str(params)] = {\n",
    "        \"alpha\": alpha,\n",
    "        \"min_iters_for_rate\": min_iters_for_rate,\n",
    "        \"max_iters_without_improv\": max_idle,\n",
    "        \"avg_frac_improvement\": avg_frac_impr,\n",
    "        \"min_frac_improvement\": min_frac_impr,\n",
    "        \"avg_frac_time\": avg_frac_time,\n",
    "        \"num_instances\": len(stats),\n",
    "    }\n",
    "    points = [(s[\"frac_time\"], s[\"frac_improvement\"]) for s in stats]\n",
    "    plot_frac_time_vs_improvement(\n",
    "        points,\n",
    "        title=f\"Global tangent (alpha={alpha}, min_iters={min_iters_for_rate}, max_idle={max_idle})\",\n",
    "        save=True,\n",
    "        save_dir=FIG_SAVE_DIR,\n",
    "    )\n",
    "\n",
    "_stats_out_dir = FIG_SAVE_DIR\n",
    "_ensure_dir(_stats_out_dir)\n",
    "\n",
    "with open(os.path.join(_stats_out_dir, \"stopping_criteria_summary.json\"), \"w\") as f:\n",
    "    json.dump(stopping_criteria_summary, f, indent=2)\n",
    "\n",
    "with open(os.path.join(_stats_out_dir, \"rate_summary.json\"), \"w\") as f:\n",
    "    json.dump(rate_summary, f, indent=2)\n",
    "\n",
    "with open(os.path.join(_stats_out_dir, \"iter_rate_summary.json\"), \"w\") as f:\n",
    "    json.dump(iter_rate_summary, f, indent=2)\n",
    "\n",
    "with open(os.path.join(_stats_out_dir, \"global_summary.json\"), \"w\") as f:\n",
    "    json.dump(global_summary, f, indent=2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
